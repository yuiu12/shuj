{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构建您的 Market Mood Reader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键组件：\n",
    "\n",
    "情感分析引擎\n",
    "数据收集和处理管道\n",
    "交易策略集成\n",
    "实时实施系统\n",
    "监控和报告仪表板"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据收集：数字挖掘的艺术"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据来源：\n",
    "\n",
    "社交媒体：\n",
    "Twitter：使用 Twitter API 流式传输与特定股票或市场趋势相关的实时推文。\n",
    "Reddit：抓取相关的 subreddit，如 r/wallstreetbets 或 r/investing，了解散户投资者的情绪。\n",
    "2. 新闻文章：\n",
    "\n",
    "RSS 源：设置来自主要财经新闻媒体（例如，路透社、彭博社、CNBC）的源。\n",
    "Web Scraping：为金融网站开发爬虫，确保符合 robots.txt 文件。\n",
    "3. 财务报告：\n",
    "\n",
    "SEC EDGAR 数据库：访问和解析 10-K 和 10-Q 报告以获取基本数据。\n",
    "财报电话会议记录：获取和分析季度财报电话会议记录。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio \n",
    "import aiohttp \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "async def  fetch_data(session,url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.text() \n",
    "async def main():\n",
    "    urls = ['url1','url2','url3'] \n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks =[fetch_data(session,url) for url in urls] \n",
    "        results = await asyncio.gather(*tasks) \n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据预处理：提炼数字黄金"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "预处理步骤：\n",
    "\n",
    "文本清理：\n",
    "删除 URL、HTML 标记和特殊字符\n",
    "处理表情符号并在相关时转换为文本\n",
    "规范化文本（例如，小写转换）\n",
    "2. 标记化：\n",
    "\n",
    "将文本拆分为单个单词或子单词\n",
    "考虑使用 WordPiece 或 SentencePiece 等高级分词器，以更好地处理超出词汇表的单词\n",
    "3. 停用词删除：\n",
    "\n",
    "删除不带有重要情感的常用词（例如，“the”、“is”、“at”）\n",
    "创建针对财务文本定制的自定义停用词列表\n",
    "4. 词形还原/词干提取：\n",
    "\n",
    "将单词简化为基本形式（例如，“trading”、“traded”、“trades”→“trade”）\n",
    "首选词形还原而不是词干提取以获得更准确的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "import nltk \n",
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text.lower()) \n",
    "    stop_words = set(stopwords.words('english')) \n",
    "    tokens = [token for token in tokens if token.isalpha() and token not in stop_words] \n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens] \n",
    "\n",
    "text = \"Trading volumes increased significantly as investors reacted to the company's strong earnings report.\" \n",
    "processed_tokens = preprocess_text(text) \n",
    "print(processed_tokens)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\石天辰\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab')\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\石天辰\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\石天辰\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "情感分析模型：我们系统的核心"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型选项：\n",
    "\n",
    "传统机器学习：\n",
    "朴素贝叶斯\n",
    "支持向量机 （SVM）\n",
    "随机森林\n",
    "2. 深度学习：\n",
    "\n",
    "长短期记忆 （LSTM） 网络\n",
    "Transformer 模型（例如 BERT、FinBERT）\n",
    "特征提取：\n",
    "\n",
    "词袋 （BoW）\n",
    "TF-IDF （词频 - 逆文档频率）\n",
    "单词嵌入（Word2Vec、GloVe）\n",
    "模型训练和评估：\n",
    "\n",
    "将数据拆分为训练集、验证集和测试集（例如，70%、15%、15%）\n",
    "在训练数据上训练模型\n",
    "使用验证集进行超参数优化\n",
    "在测试集上评估最终模型性能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch\n",
    "\n",
    "# Load pre-trained FinBERT model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "# Tokenize and encode the dataset\n",
    "encoded_data = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "labels = torch.tensor(labels)  # Assuming you have labels\n",
    "# Create DataLoader\n",
    "dataset = TensorDataset(encoded_data['input_ids'], encoded_data['attention_mask'], labels)\n",
    "loader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "# Train the model\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "for epoch in range(3):\n",
    "    for batch in loader:\n",
    "        optimizer.zero_grad()\n",
    "        input_ids, attention_mask, labels = batch\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# Use the trained model for sentiment prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与交易策略整合：从情感到行动\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "关键组件：\n",
    "\n",
    "情绪评分：\n",
    "单个资产或整个市场的情绪总分\n",
    "考虑使用滚动窗口来显示情绪趋势\n",
    "2. 信号生成：\n",
    "\n",
    "定义看涨、看跌和中性情绪的阈值\n",
    "根据情绪变化创建买入/卖出信号\n",
    "3. 风险管理：\n",
    "\n",
    "根据情绪强度设置头寸大小\n",
    "实施止损和止盈水平\n",
    "4. 投资组合分配：\n",
    "\n",
    "根据多个资产的情绪调整投资组合权重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trading_signal(sentiment_score, current_position):\n",
    "    if sentiment_score > 0.6 and current_position <= 0:\n",
    "        return \"BUY\"\n",
    "    elif sentiment_score < -0.6 and current_position >= 0:\n",
    "        return \"SELL\"\n",
    "    else:\n",
    "        return \"HOLD\"\n",
    "\n",
    "def calculate_position_size(sentiment_score, max_position):\n",
    "    return abs(sentiment_score) * max_position\n",
    "\n",
    "def set_stop_loss(entry_price, sentiment_score):\n",
    "    return entry_price * (1 - 0.05 * abs(sentiment_score))\n",
    "\n",
    "# Main trading loop\n",
    "for timestamp, data in market_data.iterrows():\n",
    "    sentiment_score = get_sentiment_score(data)\n",
    "    signal = generate_trading_signal(sentiment_score, current_position)\n",
    "    \n",
    "    if signal != \"HOLD\":\n",
    "        position_size = calculate_position_size(sentiment_score, MAX_POSITION)\n",
    "        stop_loss = set_stop_loss(data['price'], sentiment_score)\n",
    "        \n",
    "        execute_trade(signal, position_size, stop_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回溯测试和优化：从过去学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回测流程：\n",
    "数据准备：\n",
    "使历史价格数据与情绪分数保持一致\n",
    "确保数据干净且无存活者偏差\n",
    "2. 战略实施：\n",
    "\n",
    "将您的交易规则转化为代码\n",
    "包括交易成本和滑点以实现现实主义\n",
    "3. 性能指标：\n",
    "\n",
    "计算夏普比率、最大回撤和总回报等关键指标\n",
    "可视化净值曲线和回撤\n",
    "4. 参数优化：\n",
    "\n",
    "使用网格搜索、遗传算法或贝叶斯优化等技术\n",
    "小心过度拟合"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import pandas as pd\n",
    "\n",
    "class SentimentStrategy(bt.Strategy):\n",
    "    params = (\n",
    "        ('sentiment_threshold', 0.5),\n",
    "        ('position_size', 100),\n",
    "    )\n",
    "    def __init__(self):\n",
    "        self.sentiment = self.datas[0].sentiment\n",
    "        self.order = None\n",
    "    def next(self):\n",
    "        if self.order:\n",
    "            return\n",
    "        if not self.position:\n",
    "            if self.sentiment[0] > self.params.sentiment_threshold:\n",
    "                self.order = self.buy(size=self.params.position_size)\n",
    "        elif self.sentiment[0] < -self.params.sentiment_threshold:\n",
    "            self.order = self.sell(size=self.position.size)\n",
    "# Load data\n",
    "data = pd.read_csv('stock_data_with_sentiment.csv', parse_dates=True, index_col='Date')\n",
    "feed = bt.feeds.PandasData(dataname=data, sentiment='Sentiment')\n",
    "# Create a cerebro entity\n",
    "cerebro = bt.Cerebro()\n",
    "# Add data feed\n",
    "cerebro.adddata(feed)\n",
    "# Add strategy\n",
    "cerebro.addstrategy(SentimentStrategy)\n",
    "# Set initial capital\n",
    "cerebro.broker.setcash(100000.0)\n",
    "# Run backtest\n",
    "print('Starting Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "cerebro.run()\n",
    "print('Final Portfolio Value: %.2f' % cerebro.broker.getvalue())\n",
    "# Plot results\n",
    "cerebro.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化技术：\n",
    "网格搜索：系统地处理多个参数组合，测试每个组合的性能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sentiment_threshold in np.arange(0.1, 1.0, 0.1):\n",
    "    for position_size in range(50, 500, 50):\n",
    "        cerebro = bt.Cerebro()\n",
    "        cerebro.adddata(feed)\n",
    "        cerebro.addstrategy(SentimentStrategy,\n",
    "                            sentiment_threshold=sentiment_threshold,\n",
    "                            position_size=position_size)\n",
    "        cerebro.run()\n",
    "        # Store and compare results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗传算法：使用进化算法找到最佳参数。\n",
    "\n",
    "3. 贝叶斯优化：通过构建从参数到策略性能的函数映射的概率模型，高效搜索参数空间。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实时实施：Sentiment at Speed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据流：为市场数据和情绪来源设置实时源。考虑使用 Apache Kafka 来处理大容量数据流。\n",
    "实时情绪分析：实施您的情绪分析模型以实时处理传入数据。这可能涉及使用 Apache Flink 或 Spark Streaming 等流处理框架。\n",
    "交易逻辑：实施您的交易策略，根据实时情绪评分生成信号。\n",
    "订单执行：连接到经纪商的 API，根据您的交易信号自动下订单。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import alpaca_trade_api as tradeapi\n",
    "from textblob import TextBlob\n",
    "import tweepy\n",
    "\n",
    "# Set up Alpaca API\n",
    "api = tradeapi.REST('YOUR_API_KEY', 'YOUR_API_SECRET', base_url='https://paper-api.alpaca.markets')\n",
    "# Set up Twitter API\n",
    "auth = tweepy.OAuthHandler(\"CONSUMER_KEY\", \"CONSUMER_SECRET\")\n",
    "auth.set_access_token(\"ACCESS_TOKEN\", \"ACCESS_TOKEN_SECRET\")\n",
    "twitter_api = tweepy.API(auth)\n",
    "\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "def execute_trade(symbol, qty, side):\n",
    "    api.submit_order(\n",
    "        symbol=symbol,\n",
    "        qty=qty,\n",
    "        side=side,\n",
    "        type='market',\n",
    "        time_in_force='gtc'\n",
    "    )\n",
    "\n",
    "class MyStreamListener(tweepy.StreamListener):\n",
    "    def on_status(self, status):\n",
    "        if hasattr(status, 'retweeted_status'):\n",
    "            return\n",
    "        \n",
    "        sentiment = get_sentiment(status.text)\n",
    "        print(f\"Sentiment: {sentiment}\")\n",
    "        \n",
    "        if sentiment > 0.5:\n",
    "            execute_trade('AAPL', 10, 'buy')\n",
    "        elif sentiment < -0.5:\n",
    "        execute_trade('AAPL', 10, 'sell')\n",
    "\n",
    "myStreamListener = MyStreamListener()\n",
    "myStream = tweepy.Stream(auth = twitter_api.auth, listener=myStreamListener)\n",
    "myStream.filter(track=['AAPL'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "监控和报告：密切关注性能"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "要监控的关键指标：\n",
    "交易业绩：\n",
    "回报 （每日、每周、每月）\n",
    "夏普比率\n",
    "最大回撤\n",
    "胜/负比率\n",
    "2. 情绪指标：\n",
    "\n",
    "平均情绪得分\n",
    "情绪波动\n",
    "情绪与价格变动之间的相关性\n",
    "3. 系统健康：\n",
    "\n",
    "情绪分析中的延迟\n",
    "交易执行时间\n",
    "错误率\n",
    "4. 数据质量：\n",
    "\n",
    "数据馈送正常运行时间\n",
    "缺少数据点\n",
    "异常情绪评分"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dash\n",
    "import dash_core_components as dcc\n",
    "import dash_html_components as html\n",
    "from dash.dependencies import Input, Output\n",
    "import plotly.graph_objs as go\n",
    "import pandas as pd\n",
    "\n",
    "app = dash.Dash(__name__)\n",
    "# Assume we have functions to fetch the latest data\n",
    "from data_fetchers import get_latest_performance, get_latest_sentiment\n",
    "app.layout = html.Div([\n",
    "    html.H1('Sentiment Trading Dashboard'),\n",
    "    dcc.Graph(id='performance-graph'),\n",
    "    dcc.Graph(id='sentiment-graph'),\n",
    "    dcc.Interval(\n",
    "        id='interval-component',\n",
    "        interval=60*1000, # update every minute\n",
    "        n_intervals=0\n",
    "    )\n",
    "])\n",
    "@app.callback(Output('performance-graph', 'figure'),\n",
    "              Input('interval-component', 'n_intervals'))\n",
    "def update_performance_graph(n):\n",
    "    df = get_latest_performance()\n",
    "    return {\n",
    "        'data': [go.Scatter(x=df['date'], y=df['returns'], mode='lines')],\n",
    "        'layout': go.Layout(title='Cumulative Returns')\n",
    "    }\n",
    "@app.callback(Output('sentiment-graph', 'figure'),\n",
    "              Input('interval-component', 'n_intervals'))\n",
    "def update_sentiment_graph(n):\n",
    "    df = get_latest_sentiment()\n",
    "    return {\n",
    "        'data': [go.Scatter(x=df['date'], y=df['sentiment'], mode='lines')],\n",
    "        'layout': go.Layout(title='Average Sentiment Score')\n",
    "    }\n",
    "if __name__ == '__main__':\n",
    "    app.run_server(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "合规与道德：诚信交易"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要合规领域：\n",
    "数据隐私：确保您的数据收集和存储实践符合 GDPR 和 CCPA 等法规。\n",
    "市场操纵：您的策略不应试图影响市场情绪或利用纵的情绪。\n",
    "内线交易：在情绪分析中使用非公开信息时要谨慎。\n",
    "公平交易惯例：确保您的算法不参与可能被视为市场滥用的做法。\n",
    "道德考虑：\n",
    "透明度：对您在交易决策中使用情绪分析持开放态度。\n",
    "公平：确保您的策略不会不成比例地使任何市场参与者群体处于有利或不利地位。\n",
    "鲁棒性：您的系统应该能够抵御异常情况，并且不会导致市场不稳定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compliance_check(tweet, sentiment_score, trade_signal):\n",
    "    # Check for potential insider information\n",
    "    insider_keywords = ['insider', 'nonpublic', 'confidential']\n",
    "    if any(keyword in tweet.lower() for keyword in insider_keywords):\n",
    "        log_potential_issue(tweet, 'Potential insider information')\n",
    "        return False\n",
    "    \n",
    "    # Check for extreme sentiment that could indicate manipulation\n",
    "    if abs(sentiment_score) > 0.9:\n",
    "        log_potential_issue(tweet, 'Extreme sentiment detected')\n",
    "        return False\n",
    "    \n",
    "    # Check trading frequency to avoid market abuse\n",
    "    if trades_in_last_hour > MAX_TRADES_PER_HOUR:\n",
    "        log_potential_issue(tweet, 'Trading frequency exceeds limit')\n",
    "        return False\n",
    "    \n",
    "    return True\n",
    "\n",
    "# Use in main trading loop\n",
    "if compliance_check(tweet, sentiment_score, trade_signal):\n",
    "    execute_trade(trade_signal)\n",
    "else:\n",
    "    log_skipped_trade(tweet, sentiment_score, trade_signal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "未来的增强功能：进化您的 Crystal Ball"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高级 NLP 技术：探索 BERT 或 GPT-3 等 transformer 模型，进行更细致的情感分析。\n",
    "多模态情绪分析：将图像和视频分析与文本相结合，以获得更全面的情绪视图。\n",
    "可解释的 AI：实施技术使您的情绪分析更具可解释性，这有助于合规性和策略优化。\n",
    "其他数据源：探索新的数据源，如卫星图像或客流量数据，以补充传统的情感来源。\n",
    "强化学习：使用 RL 技术根据市场状况和情绪动态调整您的交易策略。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"ProsusAI/finbert\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"ProsusAI/finbert\")\n",
    "def get_advanced_sentiment(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    sentiment = probabilities[0].tolist()\n",
    "    return {\n",
    "        'positive': sentiment[2],\n",
    "        'negative': sentiment[0],\n",
    "        'neutral': sentiment[1]\n",
    "    }\n",
    "# Use in main trading loop\n",
    "sentiment = get_advanced_sentiment(tweet)\n",
    "if sentiment['positive'] > 0.7:\n",
    "    execute_trade('buy')\n",
    "elif sentiment['negative'] > 0.7:\n",
    "    execute_trade('sell')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
