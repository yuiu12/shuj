{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_input_text  = \"D:/xuexi/panama_papers/\"\n",
    "path_output_storage = \"storage\"\n",
    "path_output = \"outputs\"\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL =\"text-embedding-3-small\"\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(path_output_storage):\n",
    "    os.makedirs(path_output_storage)\n",
    "\n",
    "if not os.path.exists(path_output):\n",
    "    os.makedirs(path_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "documents = SimpleDirectoryReader(path_input_text).load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SchemaLLMPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 2344.86it/s]\n",
      "Extracting paths from text with schema: 100%|██████████| 18/18 [00:33<00:00,  1.83s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.07s/it]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:05<00:00,  1.50s/it]\n"
     ]
    }
   ],
   "source": [
    "kw_extractor_name = \"schema_llm\" \n",
    "#Define the possible entity types for the knowledge graph \n",
    "entities = Literal[\"PERSON\",\"COMPANY\",\"COUNTRY\",\"BANK\",\"SCANDAL\"] \n",
    "relations = Literal[\"OWNS\",\"LOCATED_IN\",\"INVOLVED_IN\"] \n",
    "schema = {\n",
    "    \"PERSON\": [\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"],\n",
    "    \"COMPANY\": [\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"],\n",
    "    \"COUNTRY\": [\"LOCATED_IN\"],\n",
    "    \"BANK\": [\"LOCATED_IN\", \"INVOLVED_IN\"],\n",
    "    \"SCANDAL\": [\"INVOLVED_IN\"],\n",
    "}\n",
    "kg_extractor = SchemaLLMPathExtractor(\n",
    "    llm = OpenAI(model=LLM_MODEL,temperature=TEMPERATURE),\n",
    "    possible_entities=entities,\n",
    "    possible_relations=relations,\n",
    "    kg_validation_schema=schema,\n",
    "    strict=True,\n",
    ")\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),\n",
    "    show_progress=True,\n",
    "    kg_extractors = [kg_extractor],\n",
    ")\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 935.59it/s]\n",
      "Extracting paths from text: 100%|██████████| 18/18 [00:13<00:00,  1.36it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.94s/it]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:05<00:00,  1.36s/it]\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "from llama_index.core.indices.property_graph import SimpleLLMPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"free_form\"\n",
    "\n",
    "# Create an instance of SimpleLLMPathExtractor\n",
    "kg_extractor = SimpleLLMPathExtractor(\n",
    "    llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE)\n",
    "    )\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 1119.38it/s]\n",
      "Extracting and inferring knowledge graph from text: 100%|██████████| 18/18 [00:34<00:00,  1.91s/it]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:04<00:00,  4.90s/it]\n",
      "Generating embeddings: 100%|██████████| 4/4 [00:05<00:00,  1.42s/it]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.property_graph import DynamicLLMPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"dynamic_llm\"\n",
    "\n",
    "# Define the possible entity types for the knowledge graph\n",
    "entities = [\"PERSON\", \"COMPANY\", \"COUNTRY\", \"BANK\", \"SCANDAL\"]\n",
    "\n",
    "# Define the possible relations between the entities in the knowledge graph\n",
    "relations = [\"OWNS\", \"LOCATED_IN\", \"INVOLVED_IN\"]\n",
    "\n",
    "# Create an instance of SimpleLLMPathExtractor\n",
    "kg_extractor = DynamicLLMPathExtractor(\n",
    "    llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE),\n",
    "    allowed_entity_types=entities,\n",
    "    allowed_relation_types=relations,\n",
    "    )\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing nodes: 100%|██████████| 18/18 [00:00<00:00, 1883.58it/s]\n",
      "Extracting implicit paths: 100%|██████████| 18/18 [00:00<?, ?it/s]\n",
      "Generating embeddings: 100%|██████████| 1/1 [00:05<00:00,  5.19s/it]\n",
      "Generating embeddings: 0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.property_graph import ImplicitPathExtractor\n",
    "from llama_index.core import PropertyGraphIndex\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Define a name for the keyword extractor\n",
    "kw_extractor_name = \"implicit\"\n",
    "\n",
    "# Create an instance of SimpleLLMPathExtractor\n",
    "kg_extractor = ImplicitPathExtractor()\n",
    "\n",
    "# Create a PropertyGraphIndex from the provided documents, using the specified embedding model\n",
    "index = PropertyGraphIndex.from_documents(\n",
    "    documents,  # The input documents to be processed and indexed\n",
    "    embed_model=OpenAIEmbedding(model_name=EMBEDDING_MODEL),  # Use OpenAI's embedding model for document representation\n",
    "    show_progress=True,  # Display progress during indexing\n",
    "    kg_extractors=[kg_extractor],  # Use the previously defined SchemaLLMPathExtractor for extracting knowledge graph paths\n",
    ")\n",
    "\n",
    "# Define the storage path for the keyword extractor\n",
    "path_output_storage_kg_extractor = f\"{path_output_storage}/{kw_extractor_name}/\"\n",
    "\n",
    "# Create the storage directory if it doesn't already exist\n",
    "if not os.path.exists(path_output_storage_kg_extractor):\n",
    "    os.makedirs(path_output_storage_kg_extractor)\n",
    "\n",
    "# Persist the index's storage context to the specified directory\n",
    "index.storage_context.persist(persist_dir=path_output_storage_kg_extractor)\n",
    "\n",
    "# Save the knowledge graph as a NetworkX graph to an HTML file\n",
    "index.property_graph_store.save_networkx_graph(name=f\"{path_output}/kg_{kw_extractor_name}.html\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
