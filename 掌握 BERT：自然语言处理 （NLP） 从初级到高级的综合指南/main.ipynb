{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\石天辰\\.cache\\huggingface\\hub\\models--bert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert', 'prep', '##ro', '##ces', '##sing', 'is', 'essential', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "text = \"BERT preprocessing is essential.\" \n",
    "tokens = tokenizer.tokenize(text) \n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0])\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification,BertTokenizer \n",
    "import torch \n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') \n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased') \n",
    "text = \"This movie was amazing\" \n",
    "inputs = tokenizer(text,return_tensors='pt') \n",
    "outputs = model(**inputs) \n",
    "predictions = torch.argmax(outputs.logits,dim=1) \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[[7.0430e-02, 4.7734e-02, 4.5779e-02,  ..., 9.7265e-02,\n",
      "           1.3927e-01, 3.5823e-01],\n",
      "          [3.6679e-02, 8.6646e-02, 4.3685e-02,  ..., 2.3391e-01,\n",
      "           3.7266e-02, 6.5047e-02],\n",
      "          [8.5532e-02, 1.3269e-01, 3.7626e-02,  ..., 1.2145e-01,\n",
      "           1.1870e-01, 1.3301e-01],\n",
      "          ...,\n",
      "          [5.4352e-02, 1.2452e-01, 1.4481e-01,  ..., 1.2920e-01,\n",
      "           1.0262e-01, 1.0715e-01],\n",
      "          [8.2242e-02, 1.0184e-01, 9.4826e-02,  ..., 9.6448e-02,\n",
      "           1.5859e-01, 1.1109e-01],\n",
      "          [9.6005e-02, 6.8182e-02, 6.3413e-02,  ..., 1.3608e-01,\n",
      "           1.7070e-01, 1.8861e-01]],\n",
      "\n",
      "         [[6.8129e-01, 9.2499e-03, 2.2069e-01,  ..., 5.5413e-03,\n",
      "           3.5570e-02, 7.2794e-03],\n",
      "          [2.3929e-01, 5.2402e-02, 8.9530e-02,  ..., 1.1060e-01,\n",
      "           1.2288e-01, 1.3781e-01],\n",
      "          [1.9622e-02, 1.6688e-01, 1.2318e-02,  ..., 2.9411e-01,\n",
      "           1.1144e-01, 3.4867e-02],\n",
      "          ...,\n",
      "          [1.6697e-01, 9.8040e-02, 3.0382e-02,  ..., 7.0421e-02,\n",
      "           2.8582e-02, 3.3220e-01],\n",
      "          [1.4883e-03, 2.6378e-02, 1.9741e-02,  ..., 2.5711e-02,\n",
      "           3.5336e-01, 1.7653e-02],\n",
      "          [3.8389e-02, 5.4500e-02, 1.3629e-01,  ..., 1.9162e-02,\n",
      "           4.5301e-01, 3.7396e-02]],\n",
      "\n",
      "         [[7.9269e-01, 9.7243e-03, 3.8541e-02,  ..., 1.4805e-02,\n",
      "           3.1049e-02, 5.1785e-02],\n",
      "          [7.7494e-01, 9.5062e-02, 2.6061e-02,  ..., 3.7300e-04,\n",
      "           1.1090e-02, 3.0903e-02],\n",
      "          [3.7419e-02, 7.9395e-01, 3.7422e-02,  ..., 2.4120e-03,\n",
      "           2.0760e-03, 1.0802e-02],\n",
      "          ...,\n",
      "          [4.4753e-01, 2.8031e-02, 4.7458e-03,  ..., 1.9892e-02,\n",
      "           2.3260e-02, 4.3196e-02],\n",
      "          [1.6435e-01, 1.2049e-02, 1.2449e-01,  ..., 2.1587e-01,\n",
      "           2.0468e-01, 1.1033e-01],\n",
      "          [2.6475e-01, 2.2951e-04, 5.2748e-03,  ..., 1.3399e-02,\n",
      "           5.7345e-01, 1.1234e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[9.3344e-02, 2.1293e-01, 3.6972e-01,  ..., 1.3934e-01,\n",
      "           4.8599e-02, 1.7106e-02],\n",
      "          [1.9971e-01, 3.0369e-01, 6.3534e-03,  ..., 1.6547e-01,\n",
      "           7.7451e-02, 8.3545e-02],\n",
      "          [4.3213e-01, 3.7476e-02, 9.5452e-03,  ..., 5.7163e-02,\n",
      "           5.7434e-02, 2.2210e-01],\n",
      "          ...,\n",
      "          [2.1068e-01, 5.8892e-02, 6.9903e-02,  ..., 3.5517e-01,\n",
      "           3.5457e-02, 2.6466e-02],\n",
      "          [6.0695e-01, 7.9387e-02, 1.4292e-02,  ..., 4.7614e-02,\n",
      "           1.7431e-02, 1.2246e-01],\n",
      "          [4.9465e-01, 1.0663e-01, 7.7899e-02,  ..., 8.8833e-02,\n",
      "           3.9890e-02, 2.6296e-02]],\n",
      "\n",
      "         [[8.0878e-01, 4.1631e-02, 1.3958e-02,  ..., 1.0349e-02,\n",
      "           1.8809e-02, 2.5425e-02],\n",
      "          [1.3307e-01, 1.8115e-01, 5.2752e-01,  ..., 2.5687e-03,\n",
      "           1.5900e-03, 7.4033e-04],\n",
      "          [1.7291e-03, 1.7120e-02, 2.0467e-02,  ..., 1.1523e-03,\n",
      "           4.3815e-03, 3.7378e-03],\n",
      "          ...,\n",
      "          [4.0266e-01, 3.0264e-04, 9.6105e-04,  ..., 2.2562e-02,\n",
      "           3.3733e-01, 1.9817e-01],\n",
      "          [5.6587e-03, 1.0886e-03, 2.4428e-04,  ..., 1.8748e-02,\n",
      "           4.0711e-02, 9.1227e-01],\n",
      "          [2.2959e-01, 7.7883e-03, 3.2193e-03,  ..., 4.4087e-02,\n",
      "           3.0577e-01, 3.7183e-01]],\n",
      "\n",
      "         [[9.3365e-01, 8.9009e-03, 3.3519e-03,  ..., 1.7488e-03,\n",
      "           4.8441e-07, 3.1941e-02],\n",
      "          [3.2256e-01, 1.1634e-01, 7.2592e-02,  ..., 1.8775e-02,\n",
      "           2.1183e-02, 2.4878e-01],\n",
      "          [2.7640e-01, 1.4964e-01, 3.9067e-02,  ..., 2.7023e-02,\n",
      "           3.3106e-02, 8.0994e-02],\n",
      "          ...,\n",
      "          [3.7060e-01, 4.2391e-02, 8.6422e-03,  ..., 1.2797e-01,\n",
      "           2.7933e-03, 2.3018e-01],\n",
      "          [4.8763e-01, 2.4639e-02, 1.1085e-02,  ..., 9.6227e-02,\n",
      "           1.5837e-03, 2.4186e-01],\n",
      "          [7.2059e-01, 1.3161e-02, 1.8977e-02,  ..., 1.7885e-02,\n",
      "           1.5843e-03, 1.6772e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.1719e-01, 5.7415e-02, 6.8806e-02,  ..., 5.0504e-02,\n",
      "           3.5243e-01, 7.0519e-02],\n",
      "          [6.0750e-01, 8.4180e-04, 9.5185e-03,  ..., 1.9824e-02,\n",
      "           1.2169e-01, 1.6174e-01],\n",
      "          [7.0079e-01, 4.9150e-03, 1.6540e-02,  ..., 1.6276e-02,\n",
      "           6.2723e-02, 9.7840e-02],\n",
      "          ...,\n",
      "          [4.0075e-01, 1.2945e-03, 1.6290e-02,  ..., 4.3529e-02,\n",
      "           1.2346e-01, 3.0140e-01],\n",
      "          [5.3912e-01, 2.5597e-02, 4.0776e-02,  ..., 4.0968e-02,\n",
      "           1.7424e-01, 4.8594e-02],\n",
      "          [4.5893e-01, 4.9834e-02, 7.2993e-02,  ..., 4.1408e-02,\n",
      "           1.5840e-01, 6.4526e-02]],\n",
      "\n",
      "         [[3.7846e-01, 5.2864e-02, 6.8656e-02,  ..., 6.9720e-02,\n",
      "           8.8558e-02, 8.1827e-02],\n",
      "          [1.2623e-01, 1.9775e-02, 8.2403e-01,  ..., 2.6655e-04,\n",
      "           1.3587e-03, 8.2381e-04],\n",
      "          [1.0409e-01, 1.9870e-03, 5.0331e-02,  ..., 1.0109e-04,\n",
      "           2.2091e-02, 5.0935e-03],\n",
      "          ...,\n",
      "          [4.0784e-01, 3.0065e-06, 4.8751e-04,  ..., 2.6963e-03,\n",
      "           5.1807e-01, 6.2493e-02],\n",
      "          [6.3449e-01, 5.0788e-05, 6.9240e-06,  ..., 1.9460e-04,\n",
      "           2.2434e-02, 3.4000e-01],\n",
      "          [9.3470e-01, 2.7061e-04, 2.5898e-04,  ..., 8.8163e-04,\n",
      "           7.3030e-03, 5.3788e-02]],\n",
      "\n",
      "         [[8.8728e-01, 1.0007e-02, 8.3933e-03,  ..., 1.3746e-02,\n",
      "           1.9321e-02, 2.7259e-02],\n",
      "          [7.3690e-02, 1.5640e-02, 7.7766e-02,  ..., 4.8338e-02,\n",
      "           1.7450e-01, 2.5528e-02],\n",
      "          [2.1418e-01, 1.0803e-02, 5.2765e-02,  ..., 3.4310e-02,\n",
      "           1.2109e-01, 6.4913e-02],\n",
      "          ...,\n",
      "          [2.9249e-01, 6.1183e-03, 3.6216e-02,  ..., 1.8892e-02,\n",
      "           1.2483e-01, 8.3895e-02],\n",
      "          [2.4028e-01, 1.8090e-02, 9.3870e-02,  ..., 4.5049e-02,\n",
      "           7.3168e-02, 1.3917e-01],\n",
      "          [7.1769e-01, 2.4403e-02, 2.7985e-02,  ..., 2.4017e-02,\n",
      "           2.8360e-02, 8.6419e-02]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.8836e-01, 4.1117e-02, 5.2042e-02,  ..., 6.2277e-02,\n",
      "           1.6731e-01, 1.6774e-01],\n",
      "          [6.5685e-01, 1.4143e-03, 1.5203e-02,  ..., 2.0814e-02,\n",
      "           3.9548e-02, 2.0051e-01],\n",
      "          [6.1300e-01, 3.0297e-02, 5.9802e-03,  ..., 1.8025e-02,\n",
      "           4.7889e-02, 2.0537e-01],\n",
      "          ...,\n",
      "          [5.9191e-01, 4.8442e-02, 1.8433e-02,  ..., 3.4720e-03,\n",
      "           2.6285e-02, 1.7822e-01],\n",
      "          [2.3969e-01, 6.2820e-02, 5.3862e-02,  ..., 6.0917e-02,\n",
      "           8.9731e-02, 1.4887e-01],\n",
      "          [9.8424e-02, 1.3009e-01, 4.8471e-02,  ..., 1.4005e-01,\n",
      "           1.0090e-01, 1.7430e-01]],\n",
      "\n",
      "         [[6.6559e-01, 2.4125e-02, 1.6729e-02,  ..., 1.2035e-02,\n",
      "           7.2691e-02, 1.1714e-01],\n",
      "          [9.6035e-01, 5.3911e-03, 7.9799e-03,  ..., 2.4117e-04,\n",
      "           3.6138e-03, 9.8185e-03],\n",
      "          [5.4262e-01, 2.4664e-02, 6.0771e-02,  ..., 7.5548e-03,\n",
      "           1.3294e-02, 4.7563e-02],\n",
      "          ...,\n",
      "          [5.5330e-02, 2.1465e-02, 6.9541e-02,  ..., 2.0445e-02,\n",
      "           7.1904e-02, 8.9243e-02],\n",
      "          [1.9494e-01, 4.3015e-02, 4.7886e-02,  ..., 4.1952e-02,\n",
      "           8.2111e-02, 9.4941e-02],\n",
      "          [9.3651e-01, 3.5210e-03, 1.9575e-03,  ..., 2.8222e-03,\n",
      "           1.2964e-02, 2.3794e-02]],\n",
      "\n",
      "         [[2.4417e-01, 2.5771e-02, 6.5039e-02,  ..., 9.6223e-02,\n",
      "           4.9894e-02, 2.5554e-01],\n",
      "          [4.1287e-01, 1.9217e-01, 1.9049e-03,  ..., 2.5086e-02,\n",
      "           5.5372e-02, 2.8071e-01],\n",
      "          [2.0335e-01, 7.4937e-04, 5.9549e-02,  ..., 1.0907e-02,\n",
      "           4.8008e-02, 5.8969e-01],\n",
      "          ...,\n",
      "          [2.2064e-01, 8.6763e-03, 1.3461e-02,  ..., 4.4348e-01,\n",
      "           2.4035e-02, 1.5432e-01],\n",
      "          [1.8891e-01, 6.4509e-02, 9.3445e-02,  ..., 3.9031e-02,\n",
      "           2.5850e-02, 1.2973e-01],\n",
      "          [3.1171e-01, 3.9261e-02, 7.9848e-02,  ..., 1.2115e-01,\n",
      "           4.6689e-02, 1.1552e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[9.6376e-01, 5.3860e-05, 1.3507e-03,  ..., 2.8901e-05,\n",
      "           3.1440e-03, 2.9911e-02],\n",
      "          [2.7801e-08, 4.4459e-09, 1.0000e+00,  ..., 9.1102e-11,\n",
      "           5.3630e-10, 4.6290e-13],\n",
      "          [1.1873e-05, 3.5094e-06, 1.4626e-03,  ..., 3.2803e-09,\n",
      "           1.6152e-06, 7.0434e-07],\n",
      "          ...,\n",
      "          [5.1617e-08, 1.1356e-17, 8.5888e-10,  ..., 1.7657e-06,\n",
      "           1.0000e+00, 9.4953e-08],\n",
      "          [3.9647e-03, 8.7505e-08, 3.2435e-09,  ..., 7.8169e-07,\n",
      "           3.4107e-03, 9.9259e-01],\n",
      "          [9.9155e-01, 1.1832e-07, 1.9357e-05,  ..., 9.3273e-07,\n",
      "           4.0274e-04, 7.8452e-03]],\n",
      "\n",
      "         [[6.8589e-01, 3.3413e-03, 2.0398e-02,  ..., 6.3000e-03,\n",
      "           5.0652e-02, 1.9282e-01],\n",
      "          [6.4270e-01, 1.9018e-01, 3.9990e-02,  ..., 1.7120e-03,\n",
      "           1.5020e-02, 8.7083e-02],\n",
      "          [7.1844e-01, 1.0087e-01, 2.7282e-02,  ..., 6.2760e-04,\n",
      "           2.3075e-02, 1.0278e-01],\n",
      "          ...,\n",
      "          [2.7344e-01, 1.5695e-05, 2.9943e-03,  ..., 1.5730e-02,\n",
      "           7.7449e-02, 9.5423e-02],\n",
      "          [6.2287e-01, 9.8030e-04, 1.2546e-02,  ..., 4.0776e-02,\n",
      "           8.3842e-02, 1.5395e-01],\n",
      "          [8.2563e-01, 3.7088e-04, 2.7675e-03,  ..., 2.9740e-03,\n",
      "           2.6183e-02, 1.3570e-01]],\n",
      "\n",
      "         [[7.4483e-01, 2.3373e-02, 1.9159e-02,  ..., 1.1730e-02,\n",
      "           4.2600e-02, 9.7483e-02],\n",
      "          [1.8073e-01, 2.4175e-02, 1.0727e-01,  ..., 2.2918e-02,\n",
      "           1.1532e-01, 4.3875e-01],\n",
      "          [8.7658e-01, 5.2999e-03, 5.4731e-03,  ..., 2.1006e-03,\n",
      "           2.8001e-02, 5.1001e-02],\n",
      "          ...,\n",
      "          [2.3638e-01, 3.8537e-03, 1.6309e-02,  ..., 1.2781e-02,\n",
      "           1.8115e-01, 4.7432e-01],\n",
      "          [4.9362e-01, 5.4306e-02, 3.9621e-02,  ..., 3.9370e-02,\n",
      "           4.8542e-02, 8.7039e-02],\n",
      "          [4.4490e-01, 6.8519e-02, 4.3615e-02,  ..., 5.0573e-02,\n",
      "           7.5053e-02, 1.3936e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.3577e-01, 3.7341e-03, 2.3162e-02,  ..., 9.1930e-04,\n",
      "           1.9989e-02, 1.9586e-01],\n",
      "          [4.8099e-09, 1.4892e-09, 1.0000e+00,  ..., 2.7160e-11,\n",
      "           2.5836e-11, 1.4501e-13],\n",
      "          [3.1722e-06, 1.2307e-07, 2.7418e-04,  ..., 2.6006e-10,\n",
      "           2.4114e-07, 5.5992e-08],\n",
      "          ...,\n",
      "          [1.4132e-07, 7.8344e-18, 2.2347e-09,  ..., 7.7580e-06,\n",
      "           9.9999e-01, 4.5348e-07],\n",
      "          [9.2043e-04, 1.5759e-07, 2.3168e-09,  ..., 2.4428e-07,\n",
      "           1.5016e-03, 9.9757e-01],\n",
      "          [9.9592e-01, 8.0917e-08, 1.0493e-05,  ..., 6.9774e-07,\n",
      "           7.4361e-04, 3.1589e-03]],\n",
      "\n",
      "         [[5.3902e-01, 2.7051e-02, 2.3481e-02,  ..., 2.2995e-02,\n",
      "           4.9888e-02, 2.3063e-01],\n",
      "          [1.5417e-01, 4.0492e-02, 7.8347e-02,  ..., 2.1314e-02,\n",
      "           8.3052e-02, 4.7792e-01],\n",
      "          [5.3153e-01, 2.6726e-02, 3.0052e-02,  ..., 5.8644e-02,\n",
      "           6.9945e-02, 8.9186e-02],\n",
      "          ...,\n",
      "          [4.0819e-01, 2.0571e-02, 1.9689e-02,  ..., 6.3893e-02,\n",
      "           3.7285e-02, 4.9986e-02],\n",
      "          [5.9240e-01, 1.7678e-02, 1.7229e-02,  ..., 3.9718e-02,\n",
      "           2.7748e-02, 8.9917e-02],\n",
      "          [7.2888e-01, 1.6736e-02, 1.5118e-02,  ..., 6.4178e-03,\n",
      "           4.0684e-02, 9.5366e-02]],\n",
      "\n",
      "         [[9.4064e-01, 1.1164e-03, 1.8862e-03,  ..., 2.7463e-03,\n",
      "           1.0915e-02, 2.5447e-02],\n",
      "          [4.9452e-01, 6.9643e-02, 3.6558e-02,  ..., 2.3266e-02,\n",
      "           9.4133e-02, 5.7109e-02],\n",
      "          [1.7968e-01, 1.5593e-02, 1.5499e-02,  ..., 4.3781e-02,\n",
      "           5.1494e-02, 3.3841e-02],\n",
      "          ...,\n",
      "          [4.1035e-01, 5.1078e-02, 3.0688e-02,  ..., 8.7384e-02,\n",
      "           7.5482e-02, 4.8407e-02],\n",
      "          [5.9457e-01, 1.2088e-02, 3.1242e-02,  ..., 1.8539e-02,\n",
      "           6.8103e-02, 1.3377e-01],\n",
      "          [9.8666e-01, 3.2552e-04, 5.5897e-04,  ..., 4.8037e-04,\n",
      "           2.7543e-03, 5.6578e-03]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.1819e-01, 2.4021e-04, 2.3812e-03,  ..., 3.0982e-03,\n",
      "           4.2813e-02, 6.2159e-01],\n",
      "          [2.3817e-01, 1.4936e-02, 1.6785e-04,  ..., 2.3232e-04,\n",
      "           7.6597e-02, 6.6936e-01],\n",
      "          [2.3486e-01, 7.3998e-05, 1.1190e-01,  ..., 4.7998e-03,\n",
      "           1.0325e-01, 4.0184e-01],\n",
      "          ...,\n",
      "          [3.7817e-01, 1.7651e-06, 7.1736e-04,  ..., 3.5415e-02,\n",
      "           1.4511e-02, 5.7022e-01],\n",
      "          [3.4123e-01, 7.1808e-05, 1.4880e-03,  ..., 4.4400e-04,\n",
      "           5.9352e-02, 5.9327e-01],\n",
      "          [2.4888e-01, 3.5273e-03, 2.9316e-03,  ..., 5.9164e-03,\n",
      "           2.6581e-02, 6.9708e-01]],\n",
      "\n",
      "         [[4.1328e-01, 1.1473e-02, 5.3201e-03,  ..., 6.6063e-03,\n",
      "           6.7261e-02, 4.7088e-01],\n",
      "          [2.6196e-01, 2.0505e-02, 2.7073e-02,  ..., 2.0362e-02,\n",
      "           8.9526e-02, 4.7106e-01],\n",
      "          [1.5539e-01, 1.5969e-01, 4.7879e-02,  ..., 2.5724e-02,\n",
      "           5.2438e-02, 4.0080e-01],\n",
      "          ...,\n",
      "          [1.4216e-01, 1.7397e-01, 2.3892e-02,  ..., 2.8234e-02,\n",
      "           4.4215e-02, 2.7342e-01],\n",
      "          [9.5398e-02, 3.6258e-02, 8.4126e-03,  ..., 8.9445e-03,\n",
      "           3.4377e-02, 7.6365e-01],\n",
      "          [4.6522e-01, 1.8001e-03, 1.4793e-03,  ..., 2.7164e-03,\n",
      "           3.0856e-02, 4.9122e-01]],\n",
      "\n",
      "         [[8.9234e-02, 2.3027e-01, 2.8762e-02,  ..., 5.9790e-02,\n",
      "           7.0731e-02, 1.6652e-01],\n",
      "          [3.0121e-01, 3.3867e-03, 4.0217e-02,  ..., 7.1512e-03,\n",
      "           1.6852e-01, 3.8595e-01],\n",
      "          [1.7753e-01, 2.1210e-02, 1.6291e-01,  ..., 1.3296e-02,\n",
      "           1.7765e-01, 2.1142e-01],\n",
      "          ...,\n",
      "          [3.8598e-01, 1.2481e-02, 1.6286e-02,  ..., 1.4707e-02,\n",
      "           1.3744e-01, 3.3492e-01],\n",
      "          [3.1378e-01, 1.4678e-02, 2.6924e-02,  ..., 7.8712e-03,\n",
      "           2.3795e-01, 3.4961e-01],\n",
      "          [1.2302e-01, 1.1997e-02, 1.0555e-02,  ..., 6.0970e-03,\n",
      "           1.0909e-02, 7.7127e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.5265e-01, 5.7027e-03, 1.7486e-02,  ..., 5.4614e-03,\n",
      "           4.4569e-02, 1.2744e-01],\n",
      "          [8.4397e-01, 6.1484e-03, 2.1059e-02,  ..., 3.3171e-05,\n",
      "           1.0541e-03, 8.1026e-02],\n",
      "          [5.5670e-04, 1.4441e-03, 3.5673e-04,  ..., 2.8980e-04,\n",
      "           4.9143e-04, 2.6094e-03],\n",
      "          ...,\n",
      "          [5.8836e-01, 2.2008e-05, 3.6596e-04,  ..., 1.1331e-02,\n",
      "           2.3281e-01, 1.5807e-01],\n",
      "          [7.3439e-01, 9.5600e-04, 1.3270e-04,  ..., 2.0515e-03,\n",
      "           2.9129e-02, 2.2666e-01],\n",
      "          [8.4098e-01, 2.2425e-03, 9.8966e-03,  ..., 4.4955e-03,\n",
      "           3.2671e-02, 7.9496e-02]],\n",
      "\n",
      "         [[3.0936e-01, 7.4760e-03, 1.3641e-03,  ..., 3.5355e-03,\n",
      "           5.6353e-03, 6.6346e-01],\n",
      "          [1.3202e-01, 2.0597e-01, 4.4134e-03,  ..., 3.4944e-03,\n",
      "           2.8978e-03, 6.2719e-01],\n",
      "          [7.0024e-02, 5.3930e-01, 1.2183e-02,  ..., 5.5100e-02,\n",
      "           1.4296e-02, 1.3615e-01],\n",
      "          ...,\n",
      "          [1.7488e-01, 7.6786e-03, 1.4484e-01,  ..., 3.2482e-02,\n",
      "           4.3551e-02, 1.2712e-01],\n",
      "          [1.8413e-01, 4.7716e-02, 3.2517e-02,  ..., 9.1677e-02,\n",
      "           3.5186e-02, 3.3803e-01],\n",
      "          [2.7367e-01, 4.0718e-03, 4.9838e-04,  ..., 4.3536e-04,\n",
      "           4.2117e-03, 7.1199e-01]],\n",
      "\n",
      "         [[7.3529e-01, 4.8875e-03, 4.6403e-03,  ..., 4.4050e-03,\n",
      "           5.4676e-02, 1.7750e-01],\n",
      "          [7.7735e-01, 5.9640e-03, 4.7891e-03,  ..., 4.6308e-05,\n",
      "           9.1687e-03, 1.9546e-01],\n",
      "          [5.4413e-01, 6.9542e-02, 3.0825e-02,  ..., 7.3377e-04,\n",
      "           1.4560e-02, 2.5260e-01],\n",
      "          ...,\n",
      "          [1.5046e-01, 1.6650e-02, 8.1471e-03,  ..., 5.1722e-02,\n",
      "           4.2317e-02, 3.8080e-01],\n",
      "          [2.0067e-01, 6.1803e-03, 1.2904e-02,  ..., 1.2144e-01,\n",
      "           7.4101e-02, 2.6595e-01],\n",
      "          [8.1149e-01, 7.0321e-04, 1.8208e-03,  ..., 8.8191e-04,\n",
      "           1.4900e-02, 1.6464e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[5.9441e-02, 1.2289e-02, 1.7878e-03,  ..., 6.2759e-03,\n",
      "           2.2903e-02, 8.8616e-01],\n",
      "          [2.1966e-02, 4.4873e-01, 5.2295e-03,  ..., 2.3872e-02,\n",
      "           5.2476e-03, 4.6295e-01],\n",
      "          [1.5771e-03, 2.1176e-02, 4.0526e-03,  ..., 1.8710e-01,\n",
      "           3.3270e-03, 3.2470e-01],\n",
      "          ...,\n",
      "          [5.9471e-03, 7.4329e-03, 1.7175e-02,  ..., 1.4381e-01,\n",
      "           1.5709e-02, 4.9340e-01],\n",
      "          [1.9760e-02, 7.4124e-03, 1.9480e-02,  ..., 5.0348e-02,\n",
      "           1.3582e-01, 6.0250e-01],\n",
      "          [5.3735e-02, 1.4862e-03, 3.6888e-04,  ..., 1.9168e-03,\n",
      "           1.9868e-01, 7.3779e-01]],\n",
      "\n",
      "         [[1.2583e-01, 4.3283e-01, 1.2688e-02,  ..., 1.8677e-01,\n",
      "           4.0618e-02, 5.3279e-03],\n",
      "          [1.4309e-03, 4.9730e-02, 3.6397e-02,  ..., 7.2120e-02,\n",
      "           2.3110e-02, 2.4003e-01],\n",
      "          [9.8099e-03, 1.6527e-01, 5.8504e-02,  ..., 6.9127e-02,\n",
      "           2.7811e-01, 4.8136e-02],\n",
      "          ...,\n",
      "          [3.2726e-03, 8.2206e-02, 4.8487e-02,  ..., 1.0532e-02,\n",
      "           2.0013e-02, 4.3192e-01],\n",
      "          [9.7094e-03, 5.0822e-01, 6.3090e-02,  ..., 1.8548e-02,\n",
      "           1.4563e-01, 1.5209e-02],\n",
      "          [5.1556e-02, 4.7755e-02, 2.1146e-02,  ..., 2.6486e-02,\n",
      "           2.5657e-02, 6.7826e-01]],\n",
      "\n",
      "         [[4.9841e-02, 6.7992e-02, 4.8826e-02,  ..., 9.1324e-02,\n",
      "           5.4789e-02, 1.3177e-01],\n",
      "          [1.3003e-01, 3.8889e-03, 1.3439e-02,  ..., 7.9599e-03,\n",
      "           1.1043e-01, 6.9678e-01],\n",
      "          [2.0300e-01, 4.0644e-02, 6.0303e-02,  ..., 1.5940e-02,\n",
      "           1.8481e-01, 3.0445e-01],\n",
      "          ...,\n",
      "          [4.9623e-02, 5.8422e-02, 3.2363e-02,  ..., 5.0476e-03,\n",
      "           8.1790e-02, 6.6194e-01],\n",
      "          [1.8930e-01, 3.2887e-02, 1.0672e-01,  ..., 5.7420e-02,\n",
      "           1.7109e-01, 1.3087e-01],\n",
      "          [9.4427e-02, 1.6536e-02, 9.2289e-03,  ..., 5.9547e-03,\n",
      "           2.4359e-02, 8.1952e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.3663e-02, 5.8024e-02, 3.4806e-02,  ..., 1.4574e-02,\n",
      "           9.4508e-03, 7.3055e-01],\n",
      "          [1.1973e-01, 7.5054e-03, 2.4726e-02,  ..., 7.5562e-03,\n",
      "           2.1682e-02, 7.7895e-01],\n",
      "          [1.9791e-01, 1.5506e-02, 8.2548e-02,  ..., 4.8380e-02,\n",
      "           9.2171e-02, 3.5545e-01],\n",
      "          ...,\n",
      "          [4.9055e-02, 2.7340e-02, 1.7076e-02,  ..., 1.0334e-02,\n",
      "           6.5121e-02, 5.2600e-01],\n",
      "          [9.1892e-02, 2.6124e-02, 4.8702e-02,  ..., 2.1158e-02,\n",
      "           1.0273e-01, 5.4718e-01],\n",
      "          [3.1167e-02, 4.1095e-03, 1.9897e-03,  ..., 4.2581e-03,\n",
      "           8.4628e-03, 9.3611e-01]],\n",
      "\n",
      "         [[1.3551e-02, 6.1097e-03, 1.6266e-03,  ..., 2.1110e-03,\n",
      "           1.9167e-02, 9.4598e-01],\n",
      "          [2.0477e-02, 8.3806e-03, 4.4345e-03,  ..., 1.1200e-03,\n",
      "           1.4914e-02, 9.3670e-01],\n",
      "          [1.8710e-01, 2.0639e-01, 2.7040e-02,  ..., 1.8494e-03,\n",
      "           6.5131e-02, 4.5717e-01],\n",
      "          ...,\n",
      "          [4.0967e-02, 4.4211e-02, 7.5545e-02,  ..., 1.8965e-02,\n",
      "           1.0228e-02, 8.8177e-02],\n",
      "          [3.8932e-02, 3.1811e-02, 2.1059e-02,  ..., 2.1057e-01,\n",
      "           3.7394e-02, 5.0364e-02],\n",
      "          [1.1201e-02, 3.4727e-03, 1.3752e-03,  ..., 1.7260e-03,\n",
      "           8.4126e-03, 9.6570e-01]],\n",
      "\n",
      "         [[7.9037e-02, 1.4988e-01, 5.2231e-02,  ..., 6.2995e-02,\n",
      "           4.8852e-02, 4.3737e-01],\n",
      "          [8.1860e-02, 1.5553e-02, 8.8319e-02,  ..., 7.1224e-04,\n",
      "           2.6258e-03, 6.2599e-01],\n",
      "          [1.6581e-02, 7.6883e-03, 2.8901e-03,  ..., 2.1422e-03,\n",
      "           7.1063e-04, 5.2853e-02],\n",
      "          ...,\n",
      "          [1.8322e-02, 1.4533e-03, 8.5426e-04,  ..., 1.9720e-02,\n",
      "           1.3537e-01, 7.8604e-01],\n",
      "          [3.3017e-02, 2.3922e-02, 1.8145e-03,  ..., 2.7987e-02,\n",
      "           9.3880e-02, 7.9732e-01],\n",
      "          [1.5134e-02, 4.3692e-03, 5.2098e-03,  ..., 1.3316e-02,\n",
      "           1.3959e-02, 9.1972e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.0437e-02, 4.2812e-02, 6.7547e-02,  ..., 4.0156e-03,\n",
      "           6.5226e-03, 7.1936e-01],\n",
      "          [4.3398e-02, 1.4848e-02, 1.3327e-02,  ..., 4.7273e-03,\n",
      "           9.4819e-03, 7.9357e-01],\n",
      "          [4.4936e-02, 3.8499e-02, 5.5504e-02,  ..., 6.3814e-03,\n",
      "           1.5242e-02, 2.6076e-01],\n",
      "          ...,\n",
      "          [1.7986e-02, 1.0033e-03, 4.8827e-04,  ..., 7.1753e-02,\n",
      "           8.5767e-02, 7.7667e-01],\n",
      "          [4.9706e-02, 3.2626e-02, 1.1800e-02,  ..., 6.6901e-03,\n",
      "           4.2897e-02, 8.1030e-01],\n",
      "          [1.7001e-02, 2.8268e-03, 2.3385e-03,  ..., 5.1189e-03,\n",
      "           9.1134e-03, 9.4494e-01]],\n",
      "\n",
      "         [[1.5781e-02, 3.4011e-03, 3.1545e-03,  ..., 1.1829e-02,\n",
      "           7.3062e-03, 9.3771e-01],\n",
      "          [8.5873e-03, 4.5439e-03, 1.9118e-03,  ..., 2.9008e-03,\n",
      "           1.7141e-03, 9.6726e-01],\n",
      "          [5.3114e-02, 1.1849e-02, 1.5871e-02,  ..., 3.2959e-03,\n",
      "           6.4404e-03, 8.6319e-01],\n",
      "          ...,\n",
      "          [1.7763e-02, 1.5987e-04, 2.5608e-03,  ..., 1.0066e-02,\n",
      "           8.0544e-03, 9.3516e-01],\n",
      "          [2.4454e-02, 6.1130e-04, 8.3734e-04,  ..., 5.6363e-02,\n",
      "           3.8700e-02, 8.5878e-01],\n",
      "          [6.0680e-03, 1.6347e-04, 5.1151e-04,  ..., 8.9027e-04,\n",
      "           2.5613e-03, 9.8645e-01]],\n",
      "\n",
      "         [[3.7245e-02, 6.7028e-03, 1.7493e-02,  ..., 6.2275e-03,\n",
      "           2.6691e-02, 8.6902e-01],\n",
      "          [4.9735e-02, 3.7987e-03, 5.6631e-02,  ..., 5.4491e-02,\n",
      "           9.1166e-02, 5.3618e-01],\n",
      "          [3.2602e-02, 2.5877e-02, 1.4337e-01,  ..., 6.5074e-02,\n",
      "           5.0010e-02, 4.4457e-02],\n",
      "          ...,\n",
      "          [1.8716e-02, 7.2786e-03, 3.7521e-02,  ..., 4.2839e-03,\n",
      "           1.7118e-02, 8.5004e-01],\n",
      "          [1.8565e-01, 4.9806e-02, 1.8950e-01,  ..., 3.1051e-02,\n",
      "           7.3391e-02, 2.1948e-01],\n",
      "          [1.7508e-02, 2.5607e-03, 1.9039e-03,  ..., 1.0373e-03,\n",
      "           4.0299e-03, 9.6892e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[4.4524e-02, 1.1395e-01, 8.6091e-03,  ..., 2.8084e-03,\n",
      "           6.1279e-02, 7.3025e-01],\n",
      "          [9.4743e-03, 2.5544e-03, 2.8841e-01,  ..., 2.7891e-03,\n",
      "           4.6394e-04, 2.8848e-01],\n",
      "          [2.2164e-04, 2.3188e-03, 1.6050e-03,  ..., 7.2326e-04,\n",
      "           1.4585e-05, 1.2482e-02],\n",
      "          ...,\n",
      "          [4.5944e-02, 1.2769e-04, 2.5901e-04,  ..., 5.4838e-03,\n",
      "           1.7438e-01, 7.6371e-01],\n",
      "          [1.8454e-02, 1.9025e-02, 1.1386e-03,  ..., 3.0776e-03,\n",
      "           9.7977e-02, 8.5461e-01],\n",
      "          [2.0864e-01, 4.9128e-03, 6.0498e-03,  ..., 1.3544e-02,\n",
      "           7.0703e-02, 6.5044e-01]],\n",
      "\n",
      "         [[1.5409e-02, 1.4801e-02, 2.3106e-03,  ..., 3.2845e-02,\n",
      "           3.8847e-02, 8.4292e-01],\n",
      "          [1.4135e-01, 1.3365e-02, 3.1461e-03,  ..., 1.0155e-03,\n",
      "           1.1843e-02, 8.1111e-01],\n",
      "          [1.6985e-01, 4.6614e-01, 3.6359e-02,  ..., 2.8740e-03,\n",
      "           5.9960e-03, 2.0823e-01],\n",
      "          ...,\n",
      "          [4.0186e-02, 1.4227e-02, 5.1155e-03,  ..., 1.2338e-01,\n",
      "           2.4220e-02, 4.4020e-01],\n",
      "          [2.2047e-02, 2.4275e-03, 1.1114e-03,  ..., 4.1455e-01,\n",
      "           1.3465e-01, 3.5360e-01],\n",
      "          [1.6642e-02, 2.2378e-03, 5.1904e-03,  ..., 8.4546e-03,\n",
      "           1.4675e-02, 9.2652e-01]],\n",
      "\n",
      "         [[2.2632e-02, 1.4925e-03, 5.0745e-03,  ..., 5.4180e-02,\n",
      "           2.3010e-02, 8.4421e-01],\n",
      "          [2.1803e-02, 5.0992e-03, 2.8982e-03,  ..., 3.0363e-03,\n",
      "           1.0396e-02, 9.1326e-01],\n",
      "          [6.4276e-02, 2.1896e-03, 7.4141e-03,  ..., 6.5410e-01,\n",
      "           3.3725e-02, 2.3774e-02],\n",
      "          ...,\n",
      "          [3.3746e-02, 3.1563e-03, 1.4468e-03,  ..., 4.9422e-03,\n",
      "           2.7927e-02, 9.2169e-01],\n",
      "          [5.5629e-02, 2.9969e-03, 7.5978e-03,  ..., 5.5348e-02,\n",
      "           6.2770e-02, 7.5484e-01],\n",
      "          [1.2592e-02, 5.5987e-03, 4.2636e-03,  ..., 5.9420e-03,\n",
      "           8.4357e-03, 9.4374e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.4032e-03, 8.7294e-02, 7.9701e-02,  ..., 2.5838e-01,\n",
      "           1.9613e-02, 2.7443e-02],\n",
      "          [1.2459e-03, 1.2585e-02, 6.7100e-04,  ..., 1.1346e-04,\n",
      "           1.9569e-04, 9.8350e-01],\n",
      "          [6.8991e-03, 4.6267e-02, 2.6880e-02,  ..., 3.6720e-03,\n",
      "           3.4152e-03, 8.6534e-01],\n",
      "          ...,\n",
      "          [8.5820e-04, 1.1121e-03, 7.4662e-04,  ..., 2.3867e-02,\n",
      "           9.1409e-04, 9.6048e-01],\n",
      "          [1.4575e-02, 5.4018e-02, 5.8489e-02,  ..., 1.3645e-01,\n",
      "           4.5675e-02, 2.3054e-01],\n",
      "          [3.2321e-03, 2.5242e-03, 4.3907e-04,  ..., 2.0289e-03,\n",
      "           2.3327e-03, 9.8150e-01]],\n",
      "\n",
      "         [[6.7027e-02, 1.4081e-02, 2.1217e-02,  ..., 3.1673e-01,\n",
      "           7.7065e-02, 1.1653e-01],\n",
      "          [2.2050e-02, 2.6048e-03, 3.6193e-03,  ..., 2.3728e-02,\n",
      "           2.5622e-02, 8.8237e-01],\n",
      "          [6.6662e-02, 2.6462e-02, 1.0324e-02,  ..., 4.7562e-01,\n",
      "           4.7798e-02, 1.9613e-01],\n",
      "          ...,\n",
      "          [6.2826e-02, 1.6484e-02, 1.9864e-01,  ..., 3.0541e-02,\n",
      "           1.1296e-01, 2.6001e-01],\n",
      "          [3.2342e-02, 4.3546e-02, 9.7791e-02,  ..., 1.0317e-01,\n",
      "           6.2029e-02, 1.0164e-01],\n",
      "          [9.1928e-03, 2.4611e-03, 4.1846e-03,  ..., 5.0377e-03,\n",
      "           7.8204e-03, 9.5149e-01]],\n",
      "\n",
      "         [[5.8456e-03, 1.5261e-02, 8.0300e-03,  ..., 7.9180e-02,\n",
      "           6.6321e-01, 1.3091e-01],\n",
      "          [1.1099e-02, 1.8783e-02, 6.5646e-03,  ..., 1.4093e-03,\n",
      "           4.1055e-03, 9.4257e-01],\n",
      "          [9.8862e-02, 5.4397e-02, 1.8981e-02,  ..., 1.2630e-02,\n",
      "           7.4734e-02, 6.8599e-01],\n",
      "          ...,\n",
      "          [8.7081e-02, 4.7364e-03, 2.2603e-03,  ..., 3.4403e-02,\n",
      "           1.5867e-02, 7.7563e-01],\n",
      "          [2.7526e-02, 2.1012e-02, 1.3266e-02,  ..., 2.3650e-01,\n",
      "           9.9026e-02, 2.7957e-01],\n",
      "          [2.0207e-02, 3.3856e-03, 5.6845e-03,  ..., 8.9804e-03,\n",
      "           3.2381e-02, 9.1072e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[7.0559e-02, 7.4686e-03, 1.3152e-02,  ..., 1.0013e-02,\n",
      "           8.1678e-02, 7.8804e-01],\n",
      "          [5.5051e-02, 5.4468e-02, 7.9891e-03,  ..., 4.3942e-03,\n",
      "           4.9360e-02, 7.6901e-01],\n",
      "          [2.2810e-01, 4.7216e-02, 2.2002e-02,  ..., 8.6188e-03,\n",
      "           8.0481e-02, 5.2168e-01],\n",
      "          ...,\n",
      "          [9.5115e-03, 3.5862e-03, 4.6884e-03,  ..., 3.6062e-02,\n",
      "           1.9938e-02, 6.2475e-01],\n",
      "          [1.8848e-02, 2.7463e-03, 2.8507e-03,  ..., 2.8847e-02,\n",
      "           1.0279e-01, 7.6784e-01],\n",
      "          [1.6958e-02, 4.7532e-03, 1.1925e-02,  ..., 1.1438e-02,\n",
      "           2.2917e-02, 9.0121e-01]],\n",
      "\n",
      "         [[8.5154e-03, 1.7259e-02, 3.5377e-03,  ..., 1.2909e-01,\n",
      "           1.6747e-02, 7.3208e-01],\n",
      "          [6.2014e-04, 1.1516e-03, 3.0478e-04,  ..., 3.1880e-04,\n",
      "           2.1890e-03, 9.9318e-01],\n",
      "          [1.1658e-02, 2.0187e-01, 1.1290e-02,  ..., 5.0753e-03,\n",
      "           1.4326e-02, 7.3377e-01],\n",
      "          ...,\n",
      "          [1.7972e-01, 6.1539e-03, 1.1902e-02,  ..., 1.9089e-02,\n",
      "           7.0733e-02, 6.1016e-01],\n",
      "          [8.4971e-03, 1.8124e-02, 1.8896e-03,  ..., 2.5018e-01,\n",
      "           1.6844e-02, 5.8444e-01],\n",
      "          [8.7290e-03, 6.9892e-03, 4.1479e-03,  ..., 1.4616e-02,\n",
      "           1.4632e-02, 9.2735e-01]],\n",
      "\n",
      "         [[6.5714e-03, 2.7689e-02, 4.8345e-03,  ..., 2.5734e-02,\n",
      "           6.9893e-01, 1.1687e-01],\n",
      "          [5.4526e-02, 3.8481e-04, 5.5346e-04,  ..., 7.6514e-04,\n",
      "           6.3733e-03, 9.3403e-01],\n",
      "          [3.4574e-03, 9.5501e-01, 1.3167e-04,  ..., 5.1973e-04,\n",
      "           3.6482e-04, 3.1231e-02],\n",
      "          ...,\n",
      "          [1.3169e-03, 4.4444e-04, 3.4155e-04,  ..., 5.2617e-02,\n",
      "           3.2574e-03, 8.3050e-01],\n",
      "          [1.1102e-03, 6.1633e-03, 1.7675e-04,  ..., 3.4153e-01,\n",
      "           1.5216e-01, 4.6988e-01],\n",
      "          [5.0636e-03, 7.2893e-03, 6.9687e-03,  ..., 1.1551e-02,\n",
      "           1.5405e-02, 9.0018e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[2.4165e-03, 3.0353e-02, 2.1153e-02,  ..., 2.9693e-02,\n",
      "           3.8534e-02, 7.8223e-01],\n",
      "          [1.1585e-02, 4.0063e-02, 1.0405e-01,  ..., 1.3227e-02,\n",
      "           4.0260e-02, 5.4905e-01],\n",
      "          [3.2849e-02, 4.4283e-02, 1.1719e-01,  ..., 2.8254e-02,\n",
      "           9.4303e-02, 3.6592e-01],\n",
      "          ...,\n",
      "          [3.0865e-02, 2.8552e-02, 7.2111e-02,  ..., 1.4812e-02,\n",
      "           8.7786e-02, 4.7999e-01],\n",
      "          [1.0217e-02, 2.9962e-02, 5.3113e-02,  ..., 4.6935e-02,\n",
      "           1.2069e-01, 4.9442e-01],\n",
      "          [1.0528e-02, 1.3700e-03, 5.1836e-03,  ..., 5.2993e-03,\n",
      "           2.5927e-02, 9.3450e-01]],\n",
      "\n",
      "         [[8.7511e-02, 6.8543e-03, 5.1289e-02,  ..., 2.9947e-01,\n",
      "           8.8776e-02, 3.4239e-01],\n",
      "          [1.6626e-02, 2.4264e-03, 1.6781e-02,  ..., 1.3550e-02,\n",
      "           1.5977e-02, 8.3944e-01],\n",
      "          [1.6733e-02, 9.1501e-03, 5.4020e-02,  ..., 5.9431e-02,\n",
      "           2.2719e-02, 2.4849e-01],\n",
      "          ...,\n",
      "          [2.0373e-02, 2.4548e-03, 4.5843e-03,  ..., 2.4348e-02,\n",
      "           1.4045e-02, 8.8866e-01],\n",
      "          [4.2524e-02, 5.1574e-03, 3.8967e-02,  ..., 1.5637e-01,\n",
      "           6.2340e-02, 5.5848e-01],\n",
      "          [8.5158e-03, 1.1202e-03, 1.1230e-03,  ..., 3.6555e-03,\n",
      "           4.3478e-03, 9.7017e-01]],\n",
      "\n",
      "         [[8.8657e-02, 2.6060e-02, 5.1648e-03,  ..., 7.9325e-03,\n",
      "           3.2475e-02, 8.1314e-01],\n",
      "          [1.2753e-02, 1.7745e-03, 3.3091e-01,  ..., 1.1499e-02,\n",
      "           4.5285e-03, 2.7859e-01],\n",
      "          [7.3550e-03, 7.6273e-03, 1.1392e-01,  ..., 8.3621e-03,\n",
      "           9.0702e-04, 1.3083e-01],\n",
      "          ...,\n",
      "          [6.0804e-02, 6.0322e-03, 1.9176e-02,  ..., 2.0167e-02,\n",
      "           1.5461e-01, 6.8925e-01],\n",
      "          [1.4775e-01, 7.6561e-02, 8.3871e-03,  ..., 9.5695e-03,\n",
      "           5.3702e-02, 6.7536e-01],\n",
      "          [2.3477e-02, 7.7574e-03, 1.6884e-02,  ..., 1.2362e-02,\n",
      "           2.2138e-02, 8.6896e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[6.6887e-03, 4.2171e-03, 3.8262e-03,  ..., 3.1807e-02,\n",
      "           1.8064e-02, 9.0170e-01],\n",
      "          [1.2300e-02, 2.4758e-02, 1.4852e-02,  ..., 3.5177e-02,\n",
      "           2.3062e-02, 7.1957e-01],\n",
      "          [6.0067e-02, 5.8416e-03, 1.5478e-02,  ..., 2.7974e-01,\n",
      "           4.8138e-02, 4.0904e-01],\n",
      "          ...,\n",
      "          [6.8682e-02, 8.5291e-03, 8.1706e-03,  ..., 2.0277e-01,\n",
      "           8.8474e-02, 4.7300e-01],\n",
      "          [7.9315e-03, 2.5971e-03, 4.4898e-03,  ..., 1.0227e-01,\n",
      "           5.3730e-02, 7.8036e-01],\n",
      "          [1.5892e-02, 5.2824e-03, 7.3717e-03,  ..., 2.0974e-02,\n",
      "           2.7524e-02, 8.8731e-01]],\n",
      "\n",
      "         [[3.4292e-02, 7.9105e-02, 5.0165e-02,  ..., 5.2147e-02,\n",
      "           1.0842e-02, 6.2330e-01],\n",
      "          [5.6021e-03, 8.3409e-03, 1.8007e-02,  ..., 1.2863e-02,\n",
      "           4.0306e-03, 9.0422e-01],\n",
      "          [8.8781e-03, 3.8410e-02, 3.2974e-02,  ..., 5.8968e-03,\n",
      "           3.1966e-03, 1.9900e-01],\n",
      "          ...,\n",
      "          [1.9408e-02, 2.0578e-02, 9.5428e-02,  ..., 2.6602e-02,\n",
      "           1.3636e-02, 1.9786e-01],\n",
      "          [7.7138e-02, 5.9139e-02, 8.7103e-02,  ..., 6.7788e-02,\n",
      "           5.7098e-02, 3.8856e-01],\n",
      "          [2.1870e-02, 1.3174e-02, 4.3685e-03,  ..., 2.9357e-03,\n",
      "           9.3471e-03, 9.1938e-01]],\n",
      "\n",
      "         [[2.8578e-03, 1.1148e-03, 3.5348e-03,  ..., 1.4750e-02,\n",
      "           9.3853e-01, 2.9018e-02],\n",
      "          [1.1589e-02, 2.4047e-03, 6.8250e-03,  ..., 2.6482e-03,\n",
      "           6.4696e-03, 9.5213e-01],\n",
      "          [5.8980e-03, 1.1575e-01, 9.9878e-02,  ..., 3.4086e-03,\n",
      "           3.7090e-03, 6.6041e-01],\n",
      "          ...,\n",
      "          [9.5895e-03, 2.3292e-03, 2.6968e-03,  ..., 7.6874e-03,\n",
      "           2.5574e-03, 8.8918e-01],\n",
      "          [2.5009e-02, 1.2504e-02, 7.0753e-03,  ..., 2.1087e-01,\n",
      "           1.4030e-01, 5.4839e-01],\n",
      "          [1.5781e-02, 8.4026e-03, 5.8023e-03,  ..., 9.0260e-03,\n",
      "           5.7368e-02, 8.7576e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[5.1016e-02, 2.6517e-02, 3.3709e-02,  ..., 8.0550e-02,\n",
      "           1.7952e-02, 6.7167e-01],\n",
      "          [3.9226e-01, 3.2025e-02, 5.5313e-03,  ..., 8.5339e-03,\n",
      "           3.5408e-02, 5.0946e-01],\n",
      "          [1.7709e-01, 4.5389e-02, 5.8122e-02,  ..., 2.9687e-02,\n",
      "           5.7400e-02, 4.1562e-01],\n",
      "          ...,\n",
      "          [1.6906e-01, 1.0298e-02, 2.6214e-02,  ..., 4.7061e-02,\n",
      "           4.1819e-02, 6.4507e-01],\n",
      "          [1.2214e-01, 8.1775e-03, 1.6930e-02,  ..., 6.5374e-02,\n",
      "           3.5132e-02, 6.4280e-01],\n",
      "          [3.0444e-02, 3.9416e-03, 3.0806e-03,  ..., 5.4448e-03,\n",
      "           7.7464e-03, 9.3507e-01]],\n",
      "\n",
      "         [[2.8074e-02, 1.7313e-01, 3.1172e-02,  ..., 4.7915e-02,\n",
      "           2.5321e-02, 5.6498e-01],\n",
      "          [3.3623e-02, 4.9121e-02, 1.6675e-02,  ..., 4.5638e-03,\n",
      "           9.1537e-02, 7.7571e-01],\n",
      "          [2.5062e-02, 6.3863e-02, 3.8768e-02,  ..., 2.5826e-02,\n",
      "           1.3739e-01, 6.3271e-01],\n",
      "          ...,\n",
      "          [2.2959e-02, 4.4798e-02, 2.0577e-02,  ..., 8.4243e-02,\n",
      "           6.3961e-02, 7.1117e-01],\n",
      "          [9.4794e-02, 1.1968e-01, 1.2557e-02,  ..., 4.5529e-02,\n",
      "           8.5569e-02, 5.7346e-01],\n",
      "          [1.4034e-02, 3.1541e-02, 2.0499e-02,  ..., 1.9695e-02,\n",
      "           8.7308e-02, 7.6235e-01]],\n",
      "\n",
      "         [[1.4497e-02, 3.5829e-02, 4.8929e-03,  ..., 3.1708e-03,\n",
      "           7.8901e-03, 9.1577e-01],\n",
      "          [2.7219e-02, 4.6087e-03, 6.8120e-02,  ..., 4.2432e-04,\n",
      "           1.7429e-02, 7.9960e-01],\n",
      "          [5.6662e-03, 1.4021e-03, 5.6161e-02,  ..., 3.2278e-03,\n",
      "           4.3849e-03, 8.2355e-01],\n",
      "          ...,\n",
      "          [2.0064e-02, 1.5320e-03, 4.0163e-03,  ..., 1.3550e-02,\n",
      "           4.1862e-02, 8.9665e-01],\n",
      "          [1.1051e-02, 2.5443e-01, 5.1802e-03,  ..., 1.3353e-02,\n",
      "           7.6355e-03, 6.7637e-01],\n",
      "          [1.6643e-02, 1.3001e-02, 9.7505e-03,  ..., 1.0168e-02,\n",
      "           1.5544e-02, 9.0340e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[2.5553e-02, 9.8386e-03, 7.9247e-03,  ..., 3.6459e-02,\n",
      "           2.0496e-02, 8.2403e-01],\n",
      "          [3.6993e-02, 5.2851e-02, 2.6362e-01,  ..., 5.0034e-02,\n",
      "           4.9913e-02, 1.1008e-01],\n",
      "          [1.2409e-01, 6.8761e-02, 3.9016e-02,  ..., 1.7281e-01,\n",
      "           1.1096e-01, 4.8329e-02],\n",
      "          ...,\n",
      "          [1.3134e-01, 8.5260e-03, 1.6360e-02,  ..., 6.8446e-02,\n",
      "           1.3093e-01, 5.7591e-01],\n",
      "          [5.6890e-02, 1.2751e-02, 6.2138e-03,  ..., 8.1303e-02,\n",
      "           2.0398e-02, 7.7786e-01],\n",
      "          [1.9065e-02, 9.0102e-03, 9.3593e-03,  ..., 8.2069e-03,\n",
      "           2.2451e-02, 8.9354e-01]],\n",
      "\n",
      "         [[2.7168e-03, 1.7923e-02, 1.9207e-03,  ..., 3.2201e-03,\n",
      "           3.4266e-03, 9.5959e-01],\n",
      "          [6.4397e-03, 8.2877e-03, 7.1285e-03,  ..., 2.1489e-03,\n",
      "           1.1177e-02, 9.4236e-01],\n",
      "          [2.1967e-03, 9.1412e-01, 2.3409e-02,  ..., 2.6078e-03,\n",
      "           4.6303e-03, 1.7330e-02],\n",
      "          ...,\n",
      "          [1.4584e-02, 9.3613e-02, 9.6333e-02,  ..., 2.6140e-02,\n",
      "           2.3258e-02, 4.4133e-01],\n",
      "          [5.7485e-03, 3.8546e-02, 8.6915e-03,  ..., 1.5162e-02,\n",
      "           1.5529e-02, 8.8111e-01],\n",
      "          [2.8036e-03, 1.2679e-02, 2.8722e-03,  ..., 6.7353e-03,\n",
      "           5.0012e-03, 9.5475e-01]],\n",
      "\n",
      "         [[2.6519e-02, 1.7289e-03, 2.3404e-03,  ..., 5.7775e-02,\n",
      "           5.7954e-02, 8.2169e-01],\n",
      "          [9.2803e-03, 4.1387e-02, 3.7985e-02,  ..., 6.0898e-02,\n",
      "           2.4144e-02, 7.1168e-01],\n",
      "          [6.7429e-03, 3.8493e-03, 1.3441e-02,  ..., 6.1773e-01,\n",
      "           5.3519e-02, 1.3434e-01],\n",
      "          ...,\n",
      "          [6.2384e-02, 4.5844e-03, 3.1090e-02,  ..., 2.0834e-01,\n",
      "           1.6278e-01, 2.2659e-01],\n",
      "          [2.0639e-02, 2.0322e-03, 1.7107e-03,  ..., 1.0973e-01,\n",
      "           6.5523e-02, 7.5910e-01],\n",
      "          [1.4098e-02, 1.3527e-02, 8.3395e-03,  ..., 2.8446e-02,\n",
      "           3.4577e-02, 8.5798e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[3.2877e-02, 1.2358e-02, 2.2950e-02,  ..., 1.2269e-01,\n",
      "           1.0633e-01, 5.2881e-01],\n",
      "          [6.7438e-03, 2.5055e-01, 4.5160e-03,  ..., 3.3504e-03,\n",
      "           1.8436e-02, 7.0897e-01],\n",
      "          [3.5639e-03, 4.2131e-02, 6.9600e-02,  ..., 1.2864e-01,\n",
      "           1.8517e-02, 5.1108e-01],\n",
      "          ...,\n",
      "          [6.4831e-03, 4.6727e-03, 2.9943e-03,  ..., 3.7993e-01,\n",
      "           2.5860e-02, 5.3774e-01],\n",
      "          [6.2695e-03, 8.2770e-03, 4.7798e-03,  ..., 1.0615e-02,\n",
      "           5.3752e-03, 9.4027e-01],\n",
      "          [1.7566e-03, 3.7757e-02, 1.2748e-02,  ..., 3.8639e-02,\n",
      "           8.3232e-04, 8.4428e-01]],\n",
      "\n",
      "         [[1.8986e-02, 2.0932e-02, 2.6152e-02,  ..., 2.9720e-02,\n",
      "           4.7796e-02, 7.9828e-01],\n",
      "          [5.6645e-02, 3.2160e-02, 3.2768e-03,  ..., 1.0278e-03,\n",
      "           2.8393e-03, 8.9695e-01],\n",
      "          [1.6773e-01, 3.4498e-01, 4.8803e-02,  ..., 6.0765e-03,\n",
      "           4.0516e-02, 2.8427e-01],\n",
      "          ...,\n",
      "          [1.0008e-01, 4.5112e-02, 8.2076e-02,  ..., 4.5947e-02,\n",
      "           1.5984e-02, 2.6635e-01],\n",
      "          [1.5356e-01, 2.1789e-02, 1.0587e-02,  ..., 3.2347e-02,\n",
      "           1.4235e-02, 7.1539e-01],\n",
      "          [1.1619e-01, 7.6801e-02, 3.7526e-02,  ..., 1.9441e-02,\n",
      "           2.8740e-02, 6.2741e-01]],\n",
      "\n",
      "         [[9.6969e-03, 3.6031e-02, 8.7228e-03,  ..., 2.1331e-02,\n",
      "           3.2262e-02, 8.4151e-01],\n",
      "          [2.3811e-02, 3.1414e-02, 1.1339e-02,  ..., 2.1835e-02,\n",
      "           3.7994e-02, 8.0228e-01],\n",
      "          [8.9083e-02, 1.7001e-01, 3.8844e-02,  ..., 1.7388e-01,\n",
      "           7.4480e-02, 1.2823e-01],\n",
      "          ...,\n",
      "          [2.1177e-01, 2.1492e-02, 3.7816e-02,  ..., 7.8486e-02,\n",
      "           1.6050e-01, 3.0982e-01],\n",
      "          [1.2275e-02, 1.9890e-02, 7.9388e-03,  ..., 2.1827e-02,\n",
      "           2.2163e-02, 8.6782e-01],\n",
      "          [2.0131e-02, 2.1851e-01, 2.5483e-02,  ..., 8.8230e-02,\n",
      "           4.6490e-02, 3.6704e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[1.4868e-02, 1.6275e-02, 9.5256e-03,  ..., 3.2087e-02,\n",
      "           1.5596e-02, 8.7663e-01],\n",
      "          [4.0454e-02, 1.7557e-02, 6.8657e-02,  ..., 6.0090e-03,\n",
      "           5.1941e-02, 7.2175e-01],\n",
      "          [1.5156e-02, 2.5505e-02, 4.7270e-02,  ..., 1.1239e-02,\n",
      "           1.4448e-02, 8.3762e-01],\n",
      "          ...,\n",
      "          [2.0934e-02, 1.1668e-03, 4.1935e-02,  ..., 1.8998e-02,\n",
      "           1.4663e-02, 8.2161e-01],\n",
      "          [2.0315e-02, 4.7753e-03, 1.8643e-02,  ..., 2.6832e-02,\n",
      "           3.1704e-02, 8.5428e-01],\n",
      "          [4.7180e-02, 3.4965e-02, 4.6415e-02,  ..., 3.6889e-02,\n",
      "           6.3211e-02, 6.5530e-01]],\n",
      "\n",
      "         [[4.7318e-02, 7.0474e-02, 6.2467e-02,  ..., 1.0023e-01,\n",
      "           4.6743e-02, 3.6958e-01],\n",
      "          [1.2975e-01, 1.1284e-02, 4.3209e-02,  ..., 3.7827e-02,\n",
      "           2.9455e-02, 5.4316e-01],\n",
      "          [1.3820e-01, 1.8179e-01, 5.5117e-02,  ..., 1.1684e-01,\n",
      "           3.2759e-02, 8.3820e-02],\n",
      "          ...,\n",
      "          [8.7895e-02, 1.6592e-01, 4.2386e-02,  ..., 1.0061e-01,\n",
      "           5.3995e-02, 3.1516e-01],\n",
      "          [6.0234e-02, 4.7125e-03, 3.1041e-03,  ..., 6.8208e-03,\n",
      "           3.7348e-03, 9.0759e-01],\n",
      "          [5.6905e-02, 1.4084e-01, 2.8314e-02,  ..., 8.4259e-02,\n",
      "           1.7833e-02, 3.0885e-01]],\n",
      "\n",
      "         [[1.1094e-01, 1.0451e-01, 4.9226e-02,  ..., 1.3531e-01,\n",
      "           1.1808e-01, 2.7550e-01],\n",
      "          [8.2370e-02, 1.1191e-02, 2.6035e-02,  ..., 1.4622e-02,\n",
      "           5.4240e-02, 7.1468e-01],\n",
      "          [1.7015e-01, 4.4835e-02, 4.0096e-02,  ..., 4.6398e-02,\n",
      "           9.1466e-02, 4.5727e-01],\n",
      "          ...,\n",
      "          [1.2915e-01, 4.0300e-02, 2.7995e-02,  ..., 1.2155e-02,\n",
      "           6.5491e-02, 5.7252e-01],\n",
      "          [5.3756e-02, 8.2579e-02, 2.5095e-02,  ..., 3.4890e-02,\n",
      "           1.5372e-01, 5.4052e-01],\n",
      "          [1.3239e-01, 9.7254e-02, 7.0388e-02,  ..., 7.3175e-02,\n",
      "           1.9461e-01, 1.0141e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[7.4725e-02, 4.0369e-02, 5.2483e-02,  ..., 1.2404e-01,\n",
      "           9.5886e-02, 2.0174e-01],\n",
      "          [5.1233e-02, 4.1192e-02, 2.2758e-02,  ..., 2.1521e-02,\n",
      "           5.9671e-01, 1.6592e-01],\n",
      "          [9.3360e-02, 5.0104e-02, 4.3819e-02,  ..., 2.0166e-01,\n",
      "           1.8367e-01, 2.2605e-01],\n",
      "          ...,\n",
      "          [8.8503e-02, 1.9252e-02, 1.6753e-02,  ..., 1.4054e-01,\n",
      "           3.2156e-01, 2.5359e-01],\n",
      "          [2.2150e-02, 1.1556e-01, 2.9758e-02,  ..., 2.9370e-02,\n",
      "           4.3507e-01, 1.8673e-01],\n",
      "          [3.6364e-02, 5.6611e-02, 2.3184e-02,  ..., 2.7027e-02,\n",
      "           4.5547e-01, 2.8178e-01]],\n",
      "\n",
      "         [[4.8393e-02, 1.0724e-02, 9.2871e-03,  ..., 2.5427e-02,\n",
      "           6.8787e-01, 1.4856e-01],\n",
      "          [2.0588e-03, 1.6458e-01, 5.9378e-04,  ..., 4.5325e-04,\n",
      "           7.2724e-01, 1.0071e-01],\n",
      "          [1.9711e-03, 1.9456e-02, 2.1771e-03,  ..., 2.1582e-03,\n",
      "           8.7061e-01, 7.6581e-02],\n",
      "          ...,\n",
      "          [3.1657e-03, 2.2428e-03, 1.0837e-03,  ..., 4.7859e-02,\n",
      "           8.5977e-01, 6.8014e-02],\n",
      "          [5.6548e-03, 6.7517e-03, 1.0630e-03,  ..., 1.3152e-03,\n",
      "           8.7800e-01, 9.6727e-02],\n",
      "          [3.5515e-03, 1.2470e-02, 5.3480e-04,  ..., 9.8277e-04,\n",
      "           8.7123e-01, 1.0464e-01]],\n",
      "\n",
      "         [[1.7689e-02, 2.2109e-02, 1.8582e-02,  ..., 3.3924e-02,\n",
      "           5.0577e-01, 2.2664e-01],\n",
      "          [4.1026e-02, 5.7166e-02, 2.0518e-01,  ..., 8.5959e-03,\n",
      "           2.5290e-01, 1.3331e-01],\n",
      "          [2.8100e-02, 7.7436e-02, 2.2212e-01,  ..., 2.4228e-02,\n",
      "           1.5542e-01, 6.8251e-02],\n",
      "          ...,\n",
      "          [1.5210e-01, 9.0706e-03, 2.1372e-02,  ..., 1.4394e-01,\n",
      "           3.8547e-01, 1.5100e-01],\n",
      "          [3.0368e-02, 1.2080e-02, 1.1660e-02,  ..., 9.6902e-03,\n",
      "           7.3471e-01, 1.3513e-01],\n",
      "          [5.0853e-02, 2.8734e-02, 1.7646e-02,  ..., 1.3724e-02,\n",
      "           5.7658e-01, 2.3095e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[3.3801e-02, 5.6497e-02, 3.5194e-02,  ..., 4.1529e-03,\n",
      "           5.1918e-01, 2.6959e-01],\n",
      "          [7.3111e-04, 2.9825e-01, 2.8570e-03,  ..., 9.2404e-04,\n",
      "           5.7901e-01, 1.0998e-01],\n",
      "          [2.8182e-03, 9.1139e-03, 8.3838e-02,  ..., 1.5193e-03,\n",
      "           6.3358e-01, 1.4232e-01],\n",
      "          ...,\n",
      "          [1.0098e-03, 7.0954e-03, 2.5193e-03,  ..., 1.2784e-01,\n",
      "           6.9872e-01, 1.0084e-01],\n",
      "          [1.2346e-03, 1.2508e-02, 2.1658e-03,  ..., 2.2316e-03,\n",
      "           8.7458e-01, 8.6792e-02],\n",
      "          [1.7356e-03, 1.1799e-02, 2.3818e-03,  ..., 1.6580e-03,\n",
      "           8.5532e-01, 1.1246e-01]],\n",
      "\n",
      "         [[6.8573e-02, 1.8591e-03, 1.0096e-02,  ..., 6.6784e-02,\n",
      "           5.9370e-01, 1.2336e-01],\n",
      "          [1.8825e-01, 6.4855e-01, 4.3513e-03,  ..., 1.9535e-05,\n",
      "           3.0854e-03, 1.5221e-01],\n",
      "          [3.1581e-01, 1.0922e-02, 2.1125e-01,  ..., 2.6173e-03,\n",
      "           4.8567e-03, 1.9380e-01],\n",
      "          ...,\n",
      "          [2.4982e-01, 3.2957e-05, 1.3102e-03,  ..., 6.3251e-01,\n",
      "           1.3958e-03, 9.9565e-02],\n",
      "          [1.0781e-01, 1.0005e-01, 7.1154e-02,  ..., 6.9668e-02,\n",
      "           1.9496e-01, 1.3877e-01],\n",
      "          [2.7315e-01, 8.3211e-02, 3.8656e-02,  ..., 4.2482e-02,\n",
      "           1.4174e-01, 2.2813e-01]],\n",
      "\n",
      "         [[7.1597e-02, 4.7761e-02, 5.1025e-02,  ..., 1.7676e-01,\n",
      "           1.4575e-01, 1.1091e-01],\n",
      "          [1.5971e-02, 4.1557e-02, 1.7395e-02,  ..., 2.7266e-02,\n",
      "           2.2558e-01, 1.0134e-01],\n",
      "          [5.3865e-03, 1.6426e-01, 1.6449e-02,  ..., 3.9547e-02,\n",
      "           1.4008e-01, 6.5164e-02],\n",
      "          ...,\n",
      "          [4.2916e-03, 9.6036e-02, 4.1456e-02,  ..., 2.7493e-02,\n",
      "           2.9854e-01, 7.1181e-02],\n",
      "          [9.9905e-02, 1.0826e-02, 5.4325e-03,  ..., 9.7328e-03,\n",
      "           7.1365e-01, 1.2281e-01],\n",
      "          [3.6217e-02, 5.3446e-02, 1.0729e-02,  ..., 1.5470e-02,\n",
      "           6.2308e-01, 1.5527e-01]]]], grad_fn=<SoftmaxBackward0>), tensor([[[[0.2723, 0.0244, 0.0375,  ..., 0.0289, 0.2355, 0.2004],\n",
      "          [0.0592, 0.0305, 0.0167,  ..., 0.0103, 0.4481, 0.3271],\n",
      "          [0.0396, 0.0157, 0.0078,  ..., 0.0059, 0.5191, 0.3740],\n",
      "          ...,\n",
      "          [0.0772, 0.0226, 0.0196,  ..., 0.0184, 0.4500, 0.3245],\n",
      "          [0.0097, 0.0069, 0.0039,  ..., 0.0026, 0.5913, 0.3702],\n",
      "          [0.0095, 0.0060, 0.0033,  ..., 0.0021, 0.5964, 0.3696]],\n",
      "\n",
      "         [[0.1897, 0.1325, 0.0933,  ..., 0.0385, 0.0845, 0.0822],\n",
      "          [0.1070, 0.0347, 0.0351,  ..., 0.0254, 0.3226, 0.3097],\n",
      "          [0.0372, 0.0156, 0.0117,  ..., 0.0129, 0.4612, 0.3907],\n",
      "          ...,\n",
      "          [0.1137, 0.0149, 0.0386,  ..., 0.0151, 0.3804, 0.3142],\n",
      "          [0.0120, 0.0033, 0.0053,  ..., 0.0056, 0.5527, 0.3992],\n",
      "          [0.0108, 0.0029, 0.0045,  ..., 0.0043, 0.5613, 0.3982]],\n",
      "\n",
      "         [[0.0867, 0.0734, 0.1136,  ..., 0.1338, 0.0363, 0.0385],\n",
      "          [0.0170, 0.0286, 0.0265,  ..., 0.0201, 0.4154, 0.3468],\n",
      "          [0.0427, 0.0209, 0.0780,  ..., 0.0238, 0.2911, 0.2517],\n",
      "          ...,\n",
      "          [0.0082, 0.0097, 0.0237,  ..., 0.0169, 0.4662, 0.3673],\n",
      "          [0.0208, 0.0078, 0.0070,  ..., 0.0066, 0.5305, 0.3966],\n",
      "          [0.0164, 0.0066, 0.0062,  ..., 0.0056, 0.5379, 0.4008]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[0.1329, 0.0623, 0.1365,  ..., 0.0933, 0.0107, 0.0117],\n",
      "          [0.1538, 0.0403, 0.0513,  ..., 0.0170, 0.3413, 0.2782],\n",
      "          [0.0941, 0.0111, 0.0450,  ..., 0.0358, 0.3707, 0.2957],\n",
      "          ...,\n",
      "          [0.0690, 0.0061, 0.0189,  ..., 0.0616, 0.4422, 0.3312],\n",
      "          [0.0310, 0.0075, 0.0062,  ..., 0.0050, 0.5411, 0.3879],\n",
      "          [0.0315, 0.0065, 0.0055,  ..., 0.0044, 0.5459, 0.3875]],\n",
      "\n",
      "         [[0.0246, 0.1543, 0.0336,  ..., 0.0980, 0.1951, 0.2045],\n",
      "          [0.0138, 0.0135, 0.0203,  ..., 0.0187, 0.5179, 0.3535],\n",
      "          [0.0135, 0.0073, 0.0109,  ..., 0.0094, 0.5587, 0.3590],\n",
      "          ...,\n",
      "          [0.0328, 0.0149, 0.0252,  ..., 0.0886, 0.4243, 0.3011],\n",
      "          [0.0115, 0.0042, 0.0040,  ..., 0.0035, 0.5820, 0.3776],\n",
      "          [0.0098, 0.0037, 0.0033,  ..., 0.0027, 0.5874, 0.3787]],\n",
      "\n",
      "         [[0.0164, 0.0674, 0.0512,  ..., 0.0617, 0.3287, 0.2955],\n",
      "          [0.0092, 0.0759, 0.0270,  ..., 0.0045, 0.4873, 0.3610],\n",
      "          [0.0102, 0.0242, 0.0195,  ..., 0.0022, 0.5482, 0.3578],\n",
      "          ...,\n",
      "          [0.0086, 0.0071, 0.0145,  ..., 0.0386, 0.5440, 0.3374],\n",
      "          [0.0039, 0.0118, 0.0063,  ..., 0.0051, 0.6009, 0.3566],\n",
      "          [0.0033, 0.0111, 0.0052,  ..., 0.0040, 0.6076, 0.3563]]]],\n",
      "       grad_fn=<SoftmaxBackward0>))\n"
     ]
    }
   ],
   "source": [
    "#可视化注意力权重\n",
    "import torch\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT's attention mechanism is fascinating.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "attention_weights = outputs.attentions\n",
    "print(attention_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.0929, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#预训练和 MLM \n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT is a powerful language model.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "\n",
    "loss = outputs.loss\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1094,  0.2831, -0.2219,  ..., -0.4116,  0.2604,  0.4303],\n",
      "         [ 0.3717,  0.0461,  0.3014,  ..., -0.2603, -0.2263,  0.0782],\n",
      "         [-0.2072,  0.3525, -0.2668,  ..., -0.0065, -0.6920, -0.0067],\n",
      "         ...,\n",
      "         [ 0.5834,  0.4004,  0.4881,  ..., -0.1181,  0.2630, -0.1225],\n",
      "         [ 0.0716, -0.2377, -0.3874,  ...,  0.2981,  0.0436, -0.6352],\n",
      "         [ 0.8747,  0.4112, -0.3423,  ...,  0.2614, -0.4867, -0.2898]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#使用 Hugging Face Transformer 提取单词嵌入 \n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"BERT embeddings are fascinating.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, add_special_tokens=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "word_embeddings = outputs.last_hidden_state\n",
    "print(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.3527, -1.1815, -0.3027,  ..., -0.1794,  0.0872,  0.5573],\n",
      "         [-0.3768,  0.1754,  0.3395,  ..., -0.0359,  0.2162, -1.2251],\n",
      "         [ 1.8406, -0.6458,  0.5841,  ..., -0.7345,  0.7542, -0.1614],\n",
      "         ...,\n",
      "         [ 1.0414, -0.7009,  1.0362,  ...,  1.0581, -0.3068, -1.4171],\n",
      "         [-0.8934, -0.8139, -0.3154,  ..., -0.3933, -0.6383,  0.0522],\n",
      "         [ 0.0143, -0.0423, -0.0131,  ...,  0.0044, -0.0140, -0.0394]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Advanced fine-tuning with BERT.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, output_hidden_states=True)\n",
    "\n",
    "intermediate_layer = outputs.hidden_states[6]  # 7th layer\n",
    "print(intermediate_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\石天辰\\.cache\\huggingface\\hub\\models--roberta-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.0640,  0.1073, -0.0181,  ..., -0.0383, -0.0555, -0.0151],\n",
      "         [-0.0647,  0.0450, -0.0528,  ...,  0.0814, -0.1633, -0.0284],\n",
      "         [ 0.0398,  0.0657, -0.1046,  ..., -0.1364, -0.0650,  0.0560],\n",
      "         ...,\n",
      "         [ 0.0273,  0.0271, -0.0042,  ..., -0.1554, -0.0037,  0.1105],\n",
      "         [-0.0560,  0.1078, -0.0385,  ..., -0.0619, -0.0614, -0.0363],\n",
      "         [ 0.0064,  0.1383,  0.0011,  ...,  0.0994, -0.0593,  0.0169]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import RobertaTokenizer, RobertaModel\n",
    "import torch\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "model = RobertaModel.from_pretrained('roberta-base')\n",
    "\n",
    "text = \"RoBERTa is an advanced variant of BERT.\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary: [PAD]\n"
     ]
    }
   ],
   "source": [
    "#使用 BERT 和 Hugging Face Transformers 进行文本摘要 \n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "original_text = \"Long text for summarization...\"\n",
    "inputs = tokenizer(original_text, return_tensors='pt', padding=True, truncation=True)\n",
    "\n",
    "summary_logits = model(**inputs).logits\n",
    "summary = tokenizer.decode(torch.argmax(summary_logits, dim=1))\n",
    "print(\"Summary:\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用 BERT 处理长文本 \n",
    "max_seq_length = 512 \n",
    "text = \"Long text to be handled...\" \n",
    "text_chunks = [text[i:i+max_seq_length] for i in range(0,len(text),max_seq_length)] \n",
    "for chunk in text_chunks:\n",
    "    inputs = tokenizer(chunk,return_tensors='pt',padding=True,truncation=True) \n",
    "    outputs = model(**inputs) \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#资源密集型计算 \n",
    "BERT 模型，尤其是较大的模型，对计算要求很高。要解决此问题，您可以使用混合精度训练等技术，从而减少内存消耗并加快训练速度。此外，您可以考虑使用较小的模型或云资源来完成繁重的任务。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast,GradScaler \n",
    "scaler = GradScaler() \n",
    "with autocast():\n",
    "    inputs = tokenizer(text,return_tensors='pt',padding=True,truncation=True) \n",
    "    outputs = model(**inputs) \n",
    "    print(type(outputs))  # 查看outputs的类型\n",
    "    \n",
    "    if hasattr(outputs, 'loss'):\n",
    "        loss = outputs.loss  \n",
    "    else:\n",
    "        raise ValueError(\"Model output does not contain a 'loss' attribute.\")\n",
    "    loss = outputs.loss \n",
    "scaler.scale(loss).backward() \n",
    "scaler.step(optimizer) \n",
    "scaler.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "域适应 \n",
    "\n",
    "虽然 BERT 用途广泛，但它在某些域中可能无法发挥最佳性能。要解决此问题，请对特定于域的数据微调 BERT。通过将其暴露给来自目标域的文本，BERT 将学会理解特定于该字段的细微差别和术语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_data = load_domain_specific_data()  # Load domain-specific dataset\n",
    "domain_model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "train_domain(domain_model, domain_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "带有 Hugging Face Transformer 的多语言 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:159: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\石天辰\\.cache\\huggingface\\hub\\models--bert-base-multilingual-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.1740, -0.3340, -0.6609,  ...,  0.4222,  0.3596, -0.0141],\n",
      "         [ 0.4543,  0.1575, -0.2935,  ..., -0.0887,  0.3577,  0.3759],\n",
      "         [ 0.7044,  0.1681, -0.1217,  ...,  0.9337,  0.4014, -0.2957],\n",
      "         ...,\n",
      "         [-0.2753, -0.4430, -0.5777,  ...,  0.4702,  0.0326, -0.1780],\n",
      "         [-0.4562, -0.2416, -0.9867,  ...,  0.1909,  0.5182, -0.1369],\n",
      "         [-0.4752, -0.3440, -0.5101,  ...,  0.0674,  0.5783, -0.1292]]],\n",
      "       grad_fn=<NativeLayerNormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained('bert-base-multilingual-cased')\n",
    "\n",
    "text = \"BERT understands multiple languages!\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs)\n",
    "\n",
    "embeddings = outputs.last_hidden_state\n",
    "print(embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用 BERT 进行终身学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "new_data = load_latest_data()  # Load updated dataset\n",
    "for epoch in range(epochs):\n",
    "    train_lifelong(model, new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Sentiment Class: 1\n"
     ]
    }
   ],
   "source": [
    "#加载预训练的 BERT 模型 \n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "# 文本分词和编码\n",
    "# BERT 以分词形式处理文本。您需要使用 tokenizer 对文本进行分词，并为模型对其进行编码：\n",
    "text = \"BERT is amazing!\"\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "#进行预测 \n",
    "outputs = model(**inputs)\n",
    "predicted_class = torch.argmax(outputs.logits).item()\n",
    "print(\"Predicted Sentiment Class:\", predicted_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "c:\\Users\\石天辰\\.conda\\envs\\pytorch\\lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#微调 BERT \n",
    "from transformers import BertForSequenceClassification, BertTokenizer, AdamW\n",
    "import torch\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "text = \"Sample text for training.\"\n",
    "label = 1  # Assuming positive sentiment\n",
    "\n",
    "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "outputs = model(**inputs, labels=torch.tensor([label]))\n",
    "\n",
    "loss = outputs.loss\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
