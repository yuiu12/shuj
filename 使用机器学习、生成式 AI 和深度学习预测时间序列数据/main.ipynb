{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA（自回归综合移动平均线）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ARIMA 是一种用于时间序列预测的经典统计方法。它结合了自回归 （AR） 模型、差分（使数据平稳）和移动平均 （MA） 模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "from statsmodels.tsa.arima.model import ARIMA \n",
    "time_series_data = pd.read_csv('D:/xuexi/使用机器学习、生成式 AI 和深度学习预测时间序列数据/time-series-data.csv', on_bad_lines='skip' ) \n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date']) \n",
    "time_series_data.set_index('Date',inplace=True) \n",
    "model = ARIMA(time_series_data['Value'],order=(5,1,0)) \n",
    "model_fit = model.fit() \n",
    "predictions = model_fit.forecast(steps=10) \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA（季节性 ARIMA）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SARIMA 通过考虑季节性效应来扩展 ARIMA。它适用于具有季节性模式的数据，例如每月销售数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Fit SARIMA model\n",
    "model = SARIMAX(time_series_data['Value'], order=(1, 1, 1), seasonal_order=(1, 1, 1, 12))  # (p,d,q) (P,D,Q,s)\n",
    "model_fit = model.fit(disp=False)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model_fit.forecast(steps=10)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先知"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prophet 由 Facebook 开发，是一款功能强大的工具，旨在预测时间序列数据，可以处理缺失数据和异常值，并提供可靠的不确定性区间。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "import pandas as pd\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.rename(columns={'Date': 'ds', 'Value': 'y'}, inplace=True)\n",
    "\n",
    "# Fit Prophet model\n",
    "model = Prophet()\n",
    "model.fit(time_series_data)\n",
    "\n",
    "# Make future dataframe and predictions\n",
    "future = model.make_future_dataframe(periods=10)\n",
    "forecast = model.predict(future)\n",
    "print(forecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost 是一个梯度提升框架，通过将问题转换为监督式学习任务，将以前的时间步长视为特征，可用于时间序列预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for supervised learning\n",
    "def create_lag_features(data, lag=1):\n",
    "    df = data.copy()\n",
    "    for i in range(1, lag + 1):\n",
    "        df[f'lag_{i}'] = df['Value'].shift(i)\n",
    "    return df.dropna()\n",
    "\n",
    "lag = 5\n",
    "data_with_lags = create_lag_features(time_series_data, lag=lag)\n",
    "X = data_with_lags.drop('Value', axis=1)\n",
    "y = data_with_lags['Value']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Fit XGBoost model\n",
    "model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f'Mean Squared Error: {mse}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "生成式 AI 方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GAN（生成对抗网络）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, LeakyReLU, Reshape\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "X_train, y_train = create_dataset(scaled_data, time_step)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "# GAN components\n",
    "def build_generator():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_dim=time_step))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(Dense(time_step, activation='tanh'))\n",
    "    model.add(Reshape((time_step, 1)))\n",
    "    return model\n",
    "\n",
    "def build_discriminator():\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(time_step, 1)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    return model\n",
    "# Build and compile the discriminator\n",
    "discriminator = build_discriminator()\n",
    "discriminator.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5), metrics=['accuracy'])\n",
    "\n",
    "# Build the generator\n",
    "generator = build_generator()\n",
    "\n",
    "# The generator takes noise as input and generates data\n",
    "z = Input(shape=(time_step,))\n",
    "generated_data = generator(z)\n",
    "\n",
    "# For the combined model, we will only train the generator\n",
    "discriminator.trainable = False\n",
    "\n",
    "# The discriminator takes generated data as input and determines validity\n",
    "validity = discriminator(generated_data)\n",
    "\n",
    "# The combined model (stacked generator and discriminator)\n",
    "combined = Model(z, validity)\n",
    "combined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002, 0.5))\n",
    "\n",
    "# Training the GAN\n",
    "epochs = 10000\n",
    "batch_size = 32\n",
    "valid = np.ones((batch_size, 1))\n",
    "fake = np.zeros((batch_size, 1))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---------------------\n",
    "    #  Train Discriminator\n",
    "    # ---------------------\n",
    "\n",
    "    # Select a random batch of real data\n",
    "    idx = np.random.randint(0, X_train.shape[0], batch_size)\n",
    "    real_data = X_train[idx]\n",
    "\n",
    "    # Generate a batch of fake data\n",
    "    noise = np.random.normal(0, 1, (batch_size, time_step))\n",
    "    gen_data = generator.predict(noise)\n",
    "\n",
    "    # Train the discriminator\n",
    "    d_loss_real = discriminator.train_on_batch(real_data, valid)\n",
    "    d_loss_fake = discriminator.train_on_batch(gen_data, fake)\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "    # ---------------------\n",
    "    #  Train Generator\n",
    "    # ---------------------\n",
    "\n",
    "    noise = np.random.normal(0, 1, (batch_size, time_step))\n",
    "\n",
    "    # Train the generator (to have the discriminator label samples as valid)\n",
    "    g_loss = combined.train_on_batch(noise, valid)\n",
    "\n",
    "    # Print the progress\n",
    "    if epoch % 1000 == 0:\n",
    "        print(f\"{epoch} [D loss: {d_loss[0]} | D accuracy: {100*d_loss[1]}] [G loss: {g_loss}]\")\n",
    "\n",
    "# Make predictions\n",
    "noise = np.random.normal(0, 1, (1, time_step))\n",
    "generated_prediction = generator.predict(noise)\n",
    "generated_prediction = scaler.inverse_transform(generated_prediction)\n",
    "print(generated_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WaveNet 由 DeepMind 开发，是一种深度生成模型，最初是为音频生成而设计的，但已适用于时间序列预测，尤其是在音频和语音领域。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv1D, Add, Activation, Multiply, Lambda, Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for WaveNet\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define WaveNet model\n",
    "def residual_block(x, dilation_rate):\n",
    "    tanh_out = Conv1D(32, kernel_size=2, dilation_rate=dilation_rate, padding='causal', activation='tanh')(x)\n",
    "    sigm_out = Conv1D(32, kernel_size=2, dilation_rate=dilation_rate, padding='causal', activation='sigmoid')(x)\n",
    "    out = Multiply()([tanh_out, sigm_out])\n",
    "    out = Conv1D(32, kernel_size=1, padding='same')(out)\n",
    "    out = Add()([out, x])\n",
    "    return out\n",
    "\n",
    "input_layer = Input(shape=(time_step, 1))\n",
    "out = Conv1D(32, kernel_size=2, padding='causal', activation='tanh')(input_layer)\n",
    "skip_connections = []\n",
    "for i in range(10):\n",
    "    out = residual_block(out, 2**i)\n",
    "    skip_connections.append(out)\n",
    "\n",
    "out = Add()(skip_connections)\n",
    "out = Activation('relu')(out)\n",
    "out = Conv1D(1, kernel_size=1, activation='relu')(out)\n",
    "out = Flatten()(out)\n",
    "out = Dense(1)(out)\n",
    "\n",
    "model = Model(input_layer, out)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=16)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度学习方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM （长短期记忆）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM 网络是一种能够学习长期依赖关系的递归神经网络 （RNN）。由于它们能够捕获时间模式，因此被广泛用于时间序列预测。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for LSTM\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "time_step = 10 \n",
    "X_train,y_train = create_dataset(train_data,time_step) \n",
    "X_test,y_test = create_dataset(test_data,time_step) \n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(LSTM(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "model.compile(optimizer='adam',loss='mean_squared_error') \n",
    "model.fit(X_train,y_train,batch_size=1,epochs=1) \n",
    "train_predict = model.predict(X_train) \n",
    "test_predict = model.predict(X_test) \n",
    "train_predict = scaler.inverse_transform(train_predict) \n",
    "test_predict = scaler.inverse_transform(test_predict) \n",
    "print(test_predict) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU （门控循环单元）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GRU 是 LSTM 的一种变体，它更简单，并且通常对时间序列任务执行同样出色。GRU 用于对序列进行建模和捕获时间依赖关系。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import GRU, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for GRU\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build GRU model\n",
    "model = Sequential()\n",
    "model.add(GRU(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(GRU(50, return_sequences=False))\n",
    "model.add(Dense(25))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变压器模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer 以其在 NLP 任务中的成功而闻名，已被适用于时间序列预测。Temporal Fusion Transformer （TFT） 等模型利用注意力机制来有效地处理时间数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Conv1D, MaxPooling1D, Flatten, MultiHeadAttention, LayerNormalization, Dropout\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "train_size = int(len(scaled_data) * 0.8)\n",
    "train_data = scaled_data[:train_size]\n",
    "test_data = scaled_data[train_size:]\n",
    "\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "X_train, y_train = create_dataset(train_data, time_step)\n",
    "X_test, y_test = create_dataset(test_data, time_step)\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "# Build Transformer model\n",
    "model = Sequential()\n",
    "model.add(MultiHeadAttention(num_heads=4, key_dim=2, input_shape=(time_step, 1)))\n",
    "model.add(LayerNormalization())\n",
    "model.add(Dense(50, activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model.fit(X_train, y_train, batch_size=1, epochs=1)\n",
    "\n",
    "# Make predictions\n",
    "train_predict = model.predict(X_train)\n",
    "test_predict = model.predict(X_test)\n",
    "\n",
    "train_predict = scaler.inverse_transform(train_predict)\n",
    "test_predict = scaler.inverse_transform(test_predict)\n",
    "print(test_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq （序列到序列）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq 模型用于预测数据序列。它们最初是为语言翻译而开发的，通过学习从输入序列到输出序列的映射，它们对时间序列预测非常有效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for Seq2Seq\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define Seq2Seq model\n",
    "encoder_inputs = Input(shape=(time_step, 1))\n",
    "encoder = LSTM(50, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "decoder_inputs = Input(shape=(time_step, 1))\n",
    "decoder_lstm = LSTM(50, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=[state_h, state_c])\n",
    "decoder_dense = Dense(1)\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit([X, X], y, epochs=10, batch_size=16)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict([X, X])\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCN（时间卷积网络）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TCN 使用膨胀卷积来捕获时间序列数据中的长期依赖关系。它们为顺序数据建模提供了 RNN 的强大替代方案。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for TCN\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define TCN model\n",
    "model = Sequential()\n",
    "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=1, activation='relu', input_shape=(time_step, 1)))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=2, activation='relu'))\n",
    "model.add(Conv1D(filters=64, kernel_size=2, dilation_rate=4, activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=16)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "深度AR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DeepAR 由 Amazon 开发，是一种专为时间序列预测而设计的自回归循环网络。它可以处理多个时间序列，并且可以捕获复杂的模式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Flatten\n",
    "\n",
    "# Load your time series data\n",
    "time_series_data = pd.read_csv('time_series_data.csv')\n",
    "time_series_data['Date'] = pd.to_datetime(time_series_data['Date'])\n",
    "time_series_data.set_index('Date', inplace=True)\n",
    "\n",
    "# Prepare data for DeepAR-like model\n",
    "def create_dataset(dataset, time_step=1):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(dataset)-time_step-1):\n",
    "        a = dataset[i:(i+time_step), 0]\n",
    "        X.append(a)\n",
    "        Y.append(dataset[i + time_step, 0])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "time_step = 10\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "scaled_data = scaler.fit_transform(time_series_data['Value'].values.reshape(-1, 1))\n",
    "\n",
    "X, y = create_dataset(scaled_data, time_step)\n",
    "X = X.reshape(X.shape[0], X.shape[1], 1)\n",
    "\n",
    "# Define DeepAR-like model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, return_sequences=True, input_shape=(time_step, 1)))\n",
    "model.add(LSTM(50))\n",
    "model.add(Dense(1))\n",
    "\n",
    "model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y, epochs=10, batch_size=16)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X)\n",
    "predictions = scaler.inverse_transform(predictions)\n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
