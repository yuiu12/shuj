{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "# from config import OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_storage = \"storage/implicit/\"\n",
    "\n",
    "LLM_MODEL = \"gpt-4o-mini\"\n",
    "EMBEDDING_MODEL = \"text-embedding-3-small\"\n",
    "TEMPERATURE = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "\n",
    "# Load the index from the storage context\n",
    "# StorageContext.from_defaults() initializes a default storage context with a specified directory for persistent storage\n",
    "index = load_index_from_storage(\n",
    "    StorageContext.from_defaults(persist_dir=path_storage)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Import necessary modules from the llama_index library\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Import the VectorContextRetriever class from the property_graph module\n",
    "from llama_index.core.indices.property_graph import VectorContextRetriever\n",
    "# Create a sub-retriever using VectorContextRetriever\n",
    "# This will use the property graph store and vector store from the loaded index\n",
    "# The embed_model parameter specifies the model to be used for embedding queries (e.g., OpenAI's embedding model)\n",
    "sub_retriever = VectorContextRetriever(\n",
    "    index.property_graph_store,\n",
    "    vector_store = index.vector_store,\n",
    "    embed_model = OpenAIEmbedding(model_name=EMBEDDING_MODEL),\n",
    ")\n",
    "retriever = index.as_retriever(sub_retrievers=[sub_retriever]) \n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    sub_retrievers = [retriever]\n",
    ")\n",
    "print(\n",
    "    query_engine.query(\n",
    "    \"How was the International Consortium of Investigative Journalists (ICIJ) involved in the Panama Papers scandal?\"\n",
    "    ).response\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty Response\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.indices.property_graph import LLMSynonymRetriever\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding\n",
    "from llama_index.llms.openai import OpenAI\n",
    "\n",
    "# Create a sub-retriever using LLMSynonymRetriever\n",
    "# This retriever will utilize a language model (LLM) to retrieve paths with synonymous terms or related concepts\n",
    "# from the property graph store within the index\n",
    "sub_retriever = LLMSynonymRetriever(\n",
    "    index.property_graph_store,  # The property graph store from the index is used as the data source\n",
    "    llm=OpenAI(model=LLM_MODEL, temperature=TEMPERATURE),  # Initialize the LLM with specified model and temperature\n",
    "    include_text=True,  # Include the source chunk text in the retrieved paths\n",
    "    max_keywords=100,  # Maximum number of keywords to be considered for retrieval\n",
    "    path_depth=5,  # Limit the depth of the search paths to 5 levels\n",
    ")\n",
    "\n",
    "# Create a retriever from the index using the previously defined sub-retriever\n",
    "retriever = index.as_retriever(sub_retrievers=[sub_retriever])\n",
    "\n",
    "# Initialize the query engine using the retriever\n",
    "# The query engine will use the retriever(s) to process and return responses to queries\n",
    "query_engine = index.as_query_engine(\n",
    "    sub_retrievers=[retriever]\n",
    ")\n",
    "\n",
    "# Perform a query on the query engine to retrieve information about ICIJ's involvement in the Panama Papers scandal\n",
    "# The response is then printed to the console\n",
    "print(\n",
    "    query_engine.query(\n",
    "    \"Who were the main people involved in Panama Papers scandal?\"\n",
    "    ).response\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
