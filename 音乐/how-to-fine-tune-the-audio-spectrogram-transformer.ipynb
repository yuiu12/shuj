{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46ee2202-9786-41e9-a41c-8f09192e8c12",
   "metadata": {},
   "source": [
    "# Fine-Tuning the Audio Spectrogram Transformer (AST) for Audio Classification\n",
    "\n",
    "This Jupyter Notebook provides a comprehensive guide for fine-tuning the Audio Spectrogram Transformer (AST) model on your own audio classification dataset using tools from the HuggingFace ecosystem and PyTorch. The notebook covers the entire workflow, including data loading, preprocessing, applying audio augmentations, configuring the model, and setting up the training process.\n",
    "\n",
    "**Published:** 30.07.2024  \n",
    "**Author:** Marius Steger  \n",
    "**Email:** [marius.steger@renumics.com](mailto:marius.steger@renumics.com)  \n",
    "**Organization:** [Renumics](https://renumics.com/)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694e0e96-8bdf-4147-b990-9a6df1205fb2",
   "metadata": {},
   "source": [
    "## Step 1: Install Required Packages\n",
    "Before we start, install all the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f43d402-4217-44be-a44a-def251c3e77f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: audiomentations in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (0.36.0)\n",
      "Requirement already satisfied: transformers[torch] in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (4.43.1)\n",
      "Requirement already satisfied: datasets[audio] in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (2.19.1)\n",
      "Requirement already satisfied: filelock in /home/marius/.local/lib/python3.11/site-packages (from transformers[torch]) (3.15.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.24.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (2024.5.15)\n",
      "Requirement already satisfied: requests in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (4.66.4)\n",
      "Requirement already satisfied: torch in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from transformers[torch]) (0.33.0)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (16.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.3.8)\n",
      "Requirement already satisfied: pandas in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets[audio]) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (3.9.5)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.12.1)\n",
      "Requirement already satisfied: librosa in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from datasets[audio]) (0.10.2.post1)\n",
      "Requirement already satisfied: scipy<1.13,>=1.4 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from audiomentations) (1.12.0)\n",
      "Requirement already satisfied: soxr<1.0.0,>=0.3.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from audiomentations) (0.3.7)\n",
      "Requirement already satisfied: psutil in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from accelerate>=0.21.0->transformers[torch]) (6.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from aiohttp->datasets[audio]) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers[torch]) (4.11.0)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (3.0.1)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.5.1)\n",
      "Requirement already satisfied: joblib>=0.14 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.8.2)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from librosa->datasets[audio]) (1.0.8)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from requests->transformers[torch]) (2024.7.4)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from soundfile>=0.12.1->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: sympy in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (1.13.1)\n",
      "Requirement already satisfied: networkx in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (3.3)\n",
      "Requirement already satisfied: jinja2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.20.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (2.20.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (12.1.105)\n",
      "Requirement already satisfied: triton==2.3.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from torch->transformers[torch]) (2.3.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.5.82)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from pandas->datasets[audio]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from pandas->datasets[audio]) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from pandas->datasets[audio]) (2023.3)\n",
      "Requirement already satisfied: pycparser in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from cffi>=1.0->soundfile>=0.12.1->datasets[audio]) (2.22)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from numba>=0.51.0->librosa->datasets[audio]) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/marius/.local/lib/python3.11/site-packages (from pooch>=1.1->librosa->datasets[audio]) (4.2.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->datasets[audio]) (1.16.0)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from scikit-learn>=0.20.0->librosa->datasets[audio]) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages (from sympy->torch->transformers[torch]) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers[torch] datasets[audio] audiomentations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172c2b9c-0b13-4cc0-bd1f-8b7c92b73cea",
   "metadata": {},
   "source": [
    "## Step 2: Load Your Data in the Correct Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99dafcdc-91e5-4bab-8ba1-699b79522238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Audio, ClassLabel, Features, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1721b140-c598-4bb5-b8f8-17664e0d085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define class labels\n",
    "#class_labels = ClassLabel(names=[\"bang\", \"dog_bark\"])\n",
    "\n",
    "# Define features with audio and label columns\n",
    "#features = Features({\n",
    "#    \"audio\": Audio(),\n",
    "#    \"labels\": class_labels\n",
    "#})\n",
    "\n",
    "# Load data (example with a dictionary)\n",
    "#dataset = Dataset.from_dict({\n",
    "#    \"audio\": [\"/audio/fold1/7061-6-0-0.wav\", \"/audio/fold1/7383-3-0-0.wav\"],\n",
    "#    \"labels\": [0, 1],\n",
    "#}, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d33f38c5-6b4a-4fd0-8398-ad08a4ffe011",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    }
   ],
   "source": [
    "# Load a pre-existing dataset from the HuggingFace Hub\n",
    "esc50 = load_dataset(\"ashraq/esc50\", split=\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f52267-4482-4d68-bd26-c8a9b084fbfa",
   "metadata": {},
   "source": [
    "## Step 3: Preprocess the Audio Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71680e4d-cbfa-4a7d-8a2e-abaff8d5cb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import Audio, ClassLabel\n",
    "from transformers import ASTFeatureExtractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c593aecc-06c6-4220-b0dd-0463df5bead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get target value - class name mappings\n",
    "df = esc50.select_columns([\"target\", \"category\"]).to_pandas()\n",
    "class_names = df.iloc[np.unique(df[\"target\"], return_index=True)[1]][\"category\"].to_list()\n",
    "\n",
    "# cast target and audio column\n",
    "esc50 = esc50.cast_column(\"target\", ClassLabel(names=class_names))\n",
    "esc50 = esc50.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
    "\n",
    "# rename the target feature\n",
    "esc50 = esc50.rename_column(\"target\", \"labels\")\n",
    "num_labels = len(np.unique(esc50[\"labels\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7be7bc9e-5ba5-4289-9c28-71b82ddf9137",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the pretrained model and instantiate the feature extractor\n",
    "pretrained_model = \"MIT/ast-finetuned-audioset-10-10-0.4593\"\n",
    "feature_extractor = ASTFeatureExtractor.from_pretrained(pretrained_model)\n",
    "model_input_name = feature_extractor.model_input_names[0]\n",
    "SAMPLING_RATE = feature_extractor.sampling_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e636629e-6202-4a0c-a090-50023a6a829c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function\n",
    "def preprocess_audio(batch):\n",
    "    wavs = [audio[\"array\"] for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "201eb68a-da4e-41ba-aa9f-20361bd1b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we use the esc50 train split for this tutorial on how to fine-tune the AST Model\n",
    "dataset = esc50\n",
    "label2id = dataset.features[\"labels\"]._str2int  # we add the mapping from INTs to STRINGs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "83879a8a-a8f0-4a00-9ed5-63b2bd721d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split training data\n",
    "if \"test\" not in dataset:\n",
    "    dataset = dataset.train_test_split(\n",
    "        test_size=0.2, shuffle=True, seed=0, stratify_by_column=\"labels\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54d506d-6908-478d-97d1-39f159940c73",
   "metadata": {},
   "source": [
    "## Step 4: Add Audio Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b43c7e36-e564-436d-958b-15b05c3bb94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from audiomentations import Compose, AddGaussianSNR, GainTransition, Gain, ClippingDistortion, TimeStretch, PitchShift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0c394b1-1900-411a-83d7-a1132bd3d2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define audio augmentations\n",
    "audio_augmentations = Compose([\n",
    "    AddGaussianSNR(min_snr_db=10, max_snr_db=20),\n",
    "    Gain(min_gain_db=-6, max_gain_db=6),\n",
    "    GainTransition(min_gain_db=-6, max_gain_db=6, min_duration=0.01, max_duration=0.3, duration_unit=\"fraction\"),\n",
    "    ClippingDistortion(min_percentile_threshold=0, max_percentile_threshold=30, p=0.5),\n",
    "    TimeStretch(min_rate=0.8, max_rate=1.2),\n",
    "    PitchShift(min_semitones=-4, max_semitones=4),\n",
    "], p=0.8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "597c8e0e-1172-4dfd-90e8-d715b9eae1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing with augmentations\n",
    "def preprocess_audio_with_transforms(batch):\n",
    "    wavs = [audio_augmentations(audio[\"array\"], sample_rate=SAMPLING_RATE) for audio in batch[\"input_values\"]]\n",
    "    inputs = feature_extractor(wavs, sampling_rate=SAMPLING_RATE, return_tensors=\"pt\")\n",
    "    return {model_input_name: inputs.get(model_input_name), \"labels\": list(batch[\"labels\"])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bad960a-13b5-43e9-89b9-902489af986c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=feature_extractor.sampling_rate))\n",
    "dataset = dataset.rename_column(\"audio\", \"input_values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2fea2fb-d150-4e8c-be60-3500adcea678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated mean and std: -3.3504603 4.387065\n"
     ]
    }
   ],
   "source": [
    "# calculate values for normalization\n",
    "feature_extractor.do_normalize = False  # we set normalization to False in order to calculate the mean + std of the dataset\n",
    "mean = []\n",
    "std = []\n",
    "\n",
    "# we use the transformation w/o augmentation on the training dataset to calculate the mean + std\n",
    "dataset[\"train\"].set_transform(preprocess_audio, output_all_columns=False)\n",
    "for i, (audio_input, labels) in enumerate(dataset[\"train\"]):\n",
    "    cur_mean = torch.mean(dataset[\"train\"][i][audio_input])\n",
    "    cur_std = torch.std(dataset[\"train\"][i][audio_input])\n",
    "    mean.append(cur_mean)\n",
    "    std.append(cur_std)\n",
    "\n",
    "feature_extractor.mean = np.mean(mean)\n",
    "feature_extractor.std = np.mean(std)\n",
    "feature_extractor.do_normalize = True\n",
    "\n",
    "print(\"Calculated mean and std:\", feature_extractor.mean, feature_extractor.std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5898b2d0-0bcc-463e-8c27-96e7818111fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transforms\n",
    "dataset[\"train\"].set_transform(preprocess_audio_with_transforms, output_all_columns=False)\n",
    "dataset[\"test\"].set_transform(preprocess_audio, output_all_columns=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72b5447f-5add-46a1-8425-c094131cafd2",
   "metadata": {},
   "source": [
    "## Step 5: Configure and Initialize the AST for Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e49f002d-03b7-4e51-b6cd-1f4415f8b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "from transformers import ASTConfig, ASTForAudioClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d484ed33-d35d-4d3b-9b89-df37a0675ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration from the pretrained model\n",
    "config = ASTConfig.from_pretrained(pretrained_model)\n",
    "config.num_labels = num_labels\n",
    "config.label2id = label2id\n",
    "config.id2label = {v: k for k, v in label2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a400322c-e56a-4f95-8668-9307e32ce3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ASTForAudioClassification were not initialized from the model checkpoint at MIT/ast-finetuned-audioset-10-10-0.4593 and are newly initialized because the shapes did not match:\n",
      "- classifier.dense.bias: found shape torch.Size([527]) in the checkpoint and torch.Size([50]) in the model instantiated\n",
      "- classifier.dense.weight: found shape torch.Size([527, 768]) in the checkpoint and torch.Size([50, 768]) in the model instantiated\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model with the updated configuration\n",
    "model = ASTForAudioClassification.from_pretrained(pretrained_model, config=config, ignore_mismatched_sizes=True)\n",
    "model.init_weights()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee50f12a-47bc-4d31-93c1-989911e97037",
   "metadata": {},
   "source": [
    "### Setup Metrics and Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "866e564f-11ae-40ec-acb8-febee019ebe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"./runs/ast_classifier\",\n",
    "    logging_dir=f\"./logs/ast_classifier\",\n",
    "    report_to=\"tensorboard\",\n",
    "    learning_rate=5e-5,  # LEARNING RATE\n",
    "    push_to_hub=False,\n",
    "    num_train_epochs=10,  # EPOCHS\n",
    "    per_device_train_batch_size=8,  # BATCH SIZE\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    eval_steps=1,\n",
    "    save_steps=1,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",  # eval_+metric ist utilized\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "abb0e7cc-e3c0-46d2-8271-fa0eac9442c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation metrics\n",
    "accuracy = evaluate.load(\"accuracy\")\n",
    "recall = evaluate.load(\"recall\")\n",
    "precision = evaluate.load(\"precision\")\n",
    "f1 = evaluate.load(\"f1\")\n",
    "\n",
    "AVERAGE = \"macro\" if config.num_labels > 2 else \"binary\"\n",
    "\n",
    "# setup metrics function\n",
    "def compute_metrics(eval_pred):\n",
    "    # get predictions and scores\n",
    "    logits = eval_pred.predictions\n",
    "    predictions = np.argmax(logits, axis=1)\n",
    "\n",
    "    # compute metrics\n",
    "    metrics = accuracy.compute(predictions=predictions, references=eval_pred.label_ids)\n",
    "    metrics.update(precision.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(recall.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "    metrics.update(f1.compute(predictions=predictions, references=eval_pred.label_ids, average=AVERAGE))\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f469b71-00e8-4f4e-ab24-434ebcda243d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,  # we use our configured training arguments\n",
    "    train_dataset=dataset[\"train\"],\n",
    "    eval_dataset=dataset[\"test\"],\n",
    "    compute_metrics=compute_metrics,  # we the metrics function from above\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7af0820e-fa77-4f44-95ee-612896374c33",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2000' max='2000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2000/2000 12:14, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.064900</td>\n",
       "      <td>0.557234</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.888413</td>\n",
       "      <td>0.852500</td>\n",
       "      <td>0.844813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.470500</td>\n",
       "      <td>0.292247</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.918432</td>\n",
       "      <td>0.902500</td>\n",
       "      <td>0.902523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.402400</td>\n",
       "      <td>0.295535</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.935807</td>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.921613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.402100</td>\n",
       "      <td>0.304958</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.938123</td>\n",
       "      <td>0.927500</td>\n",
       "      <td>0.926955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.229600</td>\n",
       "      <td>0.268698</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.925730</td>\n",
       "      <td>0.915000</td>\n",
       "      <td>0.911636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.225600</td>\n",
       "      <td>0.191082</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.953143</td>\n",
       "      <td>0.947500</td>\n",
       "      <td>0.947632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.178400</td>\n",
       "      <td>0.268304</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>0.925158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.080200</td>\n",
       "      <td>0.175834</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.956268</td>\n",
       "      <td>0.950000</td>\n",
       "      <td>0.950184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.082000</td>\n",
       "      <td>0.141045</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.962000</td>\n",
       "      <td>0.957500</td>\n",
       "      <td>0.957410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.133586</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.959500</td>\n",
       "      <td>0.955000</td>\n",
       "      <td>0.955100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "/home/marius/miniconda3/envs/ast-audio-classification/lib/python3.11/site-packages/audiomentations/core/transforms_interface.py:62: UserWarning: Warning: input samples dtype is np.float64. Converting to np.float32\n",
      "  warnings.warn(\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n",
      "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
      "Non-default generation parameters: {'max_length': 1024}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2000, training_loss=0.4032806022465229, metrics={'train_runtime': 734.9587, 'train_samples_per_second': 21.77, 'train_steps_per_second': 2.721, 'total_flos': 1.084989898752e+18, 'train_loss': 0.4032806022465229, 'epoch': 10.0})"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start a training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d12f99-709e-46a6-90f1-f982dcb27e3d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ast-audio-classification",
   "language": "python",
   "name": "ast-audio-classification"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
